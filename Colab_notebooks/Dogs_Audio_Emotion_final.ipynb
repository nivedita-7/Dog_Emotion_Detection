{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DehL2VUtjvH",
        "outputId": "18adfc1f-c85d-4e5c-88ed-ad6ee4868dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "d6CoiVHovC42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuE_kUbIxe_8",
        "outputId": "e1231146-4d3b-4293-e0a2-96700bee23cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path= r\"/content/drive/MyDrive/Colab_Datasets/DogAudio_Emotion_Dataset\"\n",
        "dir_list = os.listdir(path)\n",
        "dir_list[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fGoAQM4ulbv",
        "outputId": "6ce76aaf-5733-4270-97b1-dcb9a24fe1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['angry_22.wav', 'happy_45.wav', 'angry_8.wav', 'angry_9.wav', 'happy_47.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy\n",
        "from tensorflow import keras\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "gisYn4jzvLHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract metadata from audio files\n",
        "def extract_metadata(folder_path):\n",
        "    metadata = []\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.wav'):\n",
        "            label, number = file.split('.')[0].split('_')\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            duration = librosa.get_duration(filename=file_path)\n",
        "            metadata.append([file, 0, duration, label])\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# Folder containing audio files\n",
        "dataset_folder = path\n",
        "\n",
        "# Extract metadata\n",
        "metadata = extract_metadata(dataset_folder)\n",
        "\n",
        "# Convert metadata to DataFrame\n",
        "df = pd.DataFrame(metadata, columns=['File_Name', 'Start_Time', 'End_Time', 'Class'])\n",
        "\n",
        "# Save metadata to CSV file\n",
        "csv_file = 'audio_metadata.csv'\n",
        "df.to_csv(csv_file, index=False)\n",
        "\n",
        "print(f\"Metadata saved to {csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S48Ry8IFeuFI",
        "outputId": "cc40fdad-e69c-4b25-ad22-642c3d9ad7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-96efd4ae13ac>:9: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
            "\tThis alias will be removed in version 1.0.\n",
            "  duration = librosa.get_duration(filename=file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata saved to audio_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df= pd.read_csv(r\"/content/audio_metadata.csv\")\n",
        "metadata_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LGjfHVPgrHAl",
        "outputId": "b44ddb96-f066-4ae3-daf3-cf831a05bf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      File_Name  Start_Time  End_Time  Class\n",
              "0  angry_22.wav           0  2.080000  angry\n",
              "1  happy_45.wav           0  5.000000  happy\n",
              "2   angry_8.wav           0  8.262676  angry\n",
              "3   angry_9.wav           0  4.000000  angry\n",
              "4  happy_47.wav           0  1.890000  happy"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-277d488a-3aa2-40e3-84c8-dd4f0f82c287\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Start_Time</th>\n",
              "      <th>End_Time</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>angry_22.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>2.080000</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>happy_45.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>angry_8.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>8.262676</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>angry_9.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>happy_47.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>1.890000</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-277d488a-3aa2-40e3-84c8-dd4f0f82c287')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-277d488a-3aa2-40e3-84c8-dd4f0f82c287 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-277d488a-3aa2-40e3-84c8-dd4f0f82c287');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4b54e413-f850-4a40-b4bc-9830e6ef4a24\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b54e413-f850-4a40-b4bc-9830e6ef4a24')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4b54e413-f850-4a40-b4bc-9830e6ef4a24 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metadata_df",
              "summary": "{\n  \"name\": \"metadata_df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"File_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"howling_17.wav\",\n          \"sad_33.wav\",\n          \"angry_19.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Start_Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"End_Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.618098132109122,\n        \"min\": 1.0,\n        \"max\": 58.896,\n        \"num_unique_values\": 142,\n        \"samples\": [\n          8.39997732426304\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"happy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkp4PwBPrshi",
        "outputId": "b7fbfaa2-e755-44e7-ed30-b62b97181358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "angry      50\n",
              "happy      50\n",
              "sad        50\n",
              "howling    50\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Classification and Data Preprocessing"
      ],
      "metadata": {
        "id": "UEnylgyNtYKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path= path+'/angry_4.wav'\n",
        "librosa_audio_data, librosa_sample_rate= librosa.load(audio_file_path)\n",
        "print(librosa_audio_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3WiPP0otg4t",
        "outputId": "d06b1a7d-62e7-437b-df0c-d93b66143eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.1189577e-08 4.3581167e-08 5.0336894e-08 ... 3.5825332e-07 3.6442219e-07\n",
            " 3.2211130e-07]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(librosa_audio_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "ZZfvP3gNusWI",
        "outputId": "c38ca715-55af-4060-aeb4-0ae61d67de1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79d32d8c3790>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAFfCAYAAAAyMY0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYS0lEQVR4nO3dd3wUdf7H8ffuJtkkpEIagUBoUqRKiVEUlQgonuX0FA9FOYVT4dTDyp0dFdt5nh4nZ8Fyilh+9oIizYZU6RDpPQkQ0vvu/P4IrCwpJLCb2d28no/HPszOfGf2Mzg7O5/5NothGIYAAAAAAIDprGYHAAAAAAAAqpGkAwAAAADgI0jSAQAAAADwESTpAAAAAAD4CJJ0AAAAAAB8BEk6AAAAAAA+giQdAAAAAAAfEWR2AGZwOp3au3evIiMjZbFYzA4HAAAAABDgDMNQYWGhkpOTZbXWXV/eLJP0vXv3KiUlxewwAAAAAADNzK5du9S2bds61zfLJD0yMlJS9T9OVFSUydEAAAAAAAJdQUGBUlJSXPloXZplkn6kiXtUVBRJOgAAAACgyRyvyzUDxwEAAAAA4CNI0gEAAAAA8BEk6QAAAAAA+AiSdAAAAAAAfARJOgAAAAAAPoIkHQAAAAAAH0GSDgAAAACAjyBJBwAAAADAR5CkAwAAAADgI0jSAQAAAADwESTpAAAAaFZW7crThLdXaFduidmhAEANQWYHAAAAADSlS6b9KEnadahEn04cbHI0AOCOmnQAAAA0S9sPFJsdAgDUQJKOgOFwGpq3MVsHisrNDgUIWPM2ZuuFuZtkGIbZoQBArT5YvlsPfbpOTifXKQD+iebuCBizlu7U3z9aq5YtQrTi/vPNDgcISH96fZkkqWfbaJ3bNcHkaACgpjvfXyVJOrNznM7vkVhv2YKyKq3enafebWOaIDIAaBhq0hEwvl2fLUnKLa4wORIg8L39804t255rdhgAUKdDJQ27H7j43z96ORIAaBySdABAo327IVtXTF8kqfrB2Me/7FFZpcPkqAAAAPwfzd0RMOh5Bphj9CuLtWFfga7Z0U6PXtrL7HAAoFodNwaMqQHA11GTDgA4KRv2FUiSPl+9z+RIAOD4rnrpZ7NDAIB6kaQjYFg8sI/NOUW654PV2nmwxAN7AwAApjl8Y1BSUaXL/vOjps3fLElaso3xNAD4NpJ04LAqh1NXTP9J7y7bpetfX2J2OIDPeG/ZLv2w6cBxy9GCFICn3PX+Kl376uKTm0bt8KYzF+/ULzvz9PTXmZ4JDgC8rEmS9GnTpik1NVWhoaFKS0vTkiV1J0DnnHOOLBZLjdfIkSNdZa6//voa60eMGNEUhwIfdjL5waer9qrz379SXkmlJGnr/mLPBAX4uQ37CnT3B6t1zauLG1TeMAy9uGCLvlmX5eXIAASy95fv1vebDmj94e40J6O8yun6e8Rz39VaJqewTO8v28UAmAB8gtcHjnv33Xc1adIkTZ8+XWlpaXruuec0fPhwZWZmKiGh5hy7H374oSoqfpsy4+DBg+rTp4/+8Ic/uJUbMWKEXnvtNdd7u93uvYNAwLv1nV/qXb85p1DzNuZoTHqqQoNtTRQVYL69eaWNKr90+yE9OXujJGn7EyOPUxoA6ncyLXQMGco7Zhq2jVmFtZa9bNpP2pNXqo1Zhbr/oh4n/qEA4AFer0l/9tlnNW7cOI0dO1Y9evTQ9OnTFR4erhkzZtRavmXLlkpKSnK95syZo/Dw8BpJut1udysXGxvr7UOBjzvRPulPHU4ojjXl8/W68Y1lcjoNZTz7nR7/cqP+c7g/27HW7slvdDID+Lr//bxDN7yxrFHbZBeUeSkaAGice/5vjfo+MkeLG9AHfc/h3/BvN2RrV26JyqscqnI4j7MVAHiHV2vSKyoqtHz5ck2ePNm1zGq1KiMjQ4sWLWrQPl599VWNGjVKLVq0cFu+YMECJSQkKDY2Vuedd54effRRtWrVqtZ9lJeXq7y83PW+oODkm07B95zow/b/LNhS6/JXf9gmSVq5O8+17JddeTXK7cot0UUv/CCJmkMElvs/Xuu1fZdUVCnEZlWQjaFRAFSrcji1eFuu+qTEeHS/3/26v8Fldxws0VlPzZcktYkJ04/3nufRWACgIbx6d3TgwAE5HA4lJia6LU9MTFRW1vH7Ky5ZskRr167VjTfe6LZ8xIgRevPNNzV37lw9+eSTWrhwoS644AI5HLX3I5o6daqio6Ndr5SUlBM/KDQ7FVX1P0lft7f+hz5F5VVynMzAN4CPeOX7rR7ZT35ppXo88LWG1dE3FEDz9OKCLRr9ymJdP6Phg7d+sXqffs2uvQn7ydpDCzkAJvF6n/ST8eqrr6pXr14aNGiQ2/JRo0a5/u7Vq5d69+6tTp06acGCBRo6dGiN/UyePFmTJk1yvS8oKCBRR4O9u3TXCW+7L79U6VPnqU9KjD6ZcKYHowKa3qNfbKix7Oqj5hs2DEOWBvQ7Wbz1oKTaB2j8cfMBZWYVauyZqbI0ZGcAAsa7y6p/b5ftOORaZtTTTu77Tfs1YeYKSd5ryVZ9XeNaBKBpebUmPS4uTjabTdnZ2W7Ls7OzlZSUVO+2xcXFmjVrlm644Ybjfk7Hjh0VFxenzZtr7y9st9sVFRXl9gIa6qNf9pzwtl+tqW4xsqqWZvJAIFh0OOE+4mQGefpx8wGNfmWxHvl8vRZtOXj8DQA0C9/9ul8vf7dVxjEXmLV7vN99kaklAZjBq0l6SEiI+vfvr7lz57qWOZ1OzZ07V+np6fVu+/7776u8vFzXXHPNcT9n9+7dOnjwoFq3bn3SMcN/LchseJ8zAL5n9Cu/TfO2m2amACRZZNGYGUv02Jcb9MPmA3WWW7zVOw/2Dh0zOjwANAWvj9gzadIkvfzyy3rjjTe0YcMG3XzzzSouLtbYsWMlSWPGjHEbWO6IV199VZdeemmNweCKiop011136eeff9b27ds1d+5cXXLJJercubOGDx/u7cMB3JRWOHSgqPz4BYFmglahALzlm3XuLTOPbgp/1Us/a/uBml1oTtY5Ty/w+D4B4Hi83if9qquu0v79+/XAAw8oKytLffv21ezZs12Dye3cuVNWq/uzgszMTP3www/65ptvauzPZrNp9erVeuONN5SXl6fk5GQNGzZMU6ZMYa50NLlBj3+rwrIqs8MAfIIhKSu/7inYcgrKNP5/y5UYxbUaQOP97+cdmnJpzzrXb84pUmpcizrXn4jCcn7jATS9Jhk4buLEiZo4cWKt6xYsWFBjWdeuXWv0OzoiLCxMX3/9tSfDAxrs2MFjjpegU6uI5qSwrKrG4HKfrdqrNXvyNfmCbnpi9katZHwGAHWo7TeztoHjPl21VzMX79C+eh4KAoA/8+nR3QFf05i5VrfuL6p36rWySodCg22eCAvwWX955xdJ0qDUliqi1QmAk7Rse65uPXxdORZjvAEIFF7vkw40hePNZV6XulpsnKz3lu7Sef9YWOuUVZL0zzm/qtv9s/XTlroHwQECycFixm4AUL/afpKP7Yf+0GfrmiiamkoqeNAIoGmQpCMgDH12QaO3ySkoU9rjc49f8ARMX7ilxrLdh0pcf/9r7iZJ0sOfrvfK5wP+4qFP16m4lj6f0+ZvlrOeligAmod/z699et2mNj8zRz0e+FrPfpNpdigAmgGSdASEXbmNn67pxYVblFPo+do9wzC0tZYRZse+ttTjnwX4srwGTF30+k/b9fzhh1ZH23GwRJ+s2uONsACg0R78pLoG//l5vvHQAEBgI0lHQNqcU3TcMifa0n3Ec9/pbx+tqXN9XfO1b2pATEAguemt5Q0qt62OaZN2HCypdTmAwFNR5dTuQ8d/4L52T0Gd66Z8Tus0AIGBJB0BaVN2Ya3LZy7eqckfrjmpZrQbswo1c/HOOtfXlXDUprZRawFv2JxTpKe/3qj8ksom+8yft+a6/raIqQ4A1O1fc3896X3szOXBHoDAwOjuCEh1pb5HasAzuic0XTCADzj/nwtlGNVdQ56/ul+d5TbnFKl9q3AF2zz7DNeQwZSEAOr0ycq9ZodQL65fAJoSNekISMdryv6/n3fo9Z+2N0ks9aF2EU3lyHeivnnKP1i+WxnPLtSf/9ewZuqNcbzGK+VVDo9/JgB4wp9eX6rSCq5RAJoONelolurqN+4JjXnaTnN3+JJXvt8qSZq3Mcfj+578Yd3jOPy05SB9SQH4LG9cEwGgPtSkIyAdSX6/WrNPl0z7UTsONryfONAclFc5VOVw1rn+0mk/NlksReVVeuvnusd5AICGenL2RrNDAICTRpKOgHbz2yu0alee7vpgtdmhAKY5Ohk3ZKiiyqn+U77VkKcX1LlNfc3iAcDTPNXn+8UFWzyzIwAwEc3d0SwUlFZqzvpss8MATHFsH/OtB4pUVF6lovIqkyICAABAXahJR0Basi23xrJxby7z+udWVDm1YV/dc7gCZphLf0oAPu54A776om/XZ+vLNfvMDgNAACJJR0B6c9EOt/eV9fS9PVFb9hfVWHbTW8v13rLdHv8swFuMw3fGhmGovMrz3xMACESVDqdufHOZbnl7hQ4VV5gdDoAAQ5KOZmHLfs8PHHfR8z/UWNbYEWD9seYA/q+2qf9ueXuFth3wrQEWn/t2k37NLjQ7DACowXHUvJJ0HQLgaSTpwAkqrWTOVPi/f367SZlZhfpqbZbZodRq0nsrzQ4BQBPw1MBxABAIGDgOMBE3JfC2zKz6a6Kfn7tJz8/d1ETRNF55JU3wAQBA80JNOgAEsOHPfVdjGQ+HAAAAfBdJOmAi+qSjqfnbOedn4QJoJvztWgrAv5CkAwAAAADgI+iTDpiIZscAgOZsc06R1u8r8LuaaX6/AXgTSTpgIn+7KQGaGvfBQGDLeHah2SGcEH6/AXgTzd0BE23KKTI7BDRD/pT4ch8MAACaG5J0wGQHi8rNDgEAAACAjyBJR8CauyHb7BAapNJBXSGazu5Dpfph8wGzwwAAAEAdSNIRsG54Y5nZIQA+6eHP1psdAgD4NYPOOAC8iIHjAJPxQw8AgP+46/1VWrwt1+wwAAQwknQACFDvLd1ldggAEHDeX77b7BAABDiauwMms/jVWNvwJ3f/32qzQwAAAEAjkaQDAAAAAOAjSNLh97YdKDY7hJNCn3QAAAAARzRJkj5t2jSlpqYqNDRUaWlpWrJkSZ1lX3/9dVksFrdXaGioWxnDMPTAAw+odevWCgsLU0ZGhjZt2uTtw4CPOveZBWaHAAAAAAAe4fUk/d1339WkSZP04IMPasWKFerTp4+GDx+unJycOreJiorSvn37XK8dO3a4rX/qqaf0/PPPa/r06Vq8eLFatGih4cOHq6yszNuHAwAAAACA13g9SX/22Wc1btw4jR07Vj169ND06dMVHh6uGTNm1LmNxWJRUlKS65WYmOhaZxiGnnvuOd1333265JJL1Lt3b7355pvau3evPv74Y28fDuBxDBwH1M0w6A4CwLf9Z8EWFZdXmR0GgADi1SS9oqJCy5cvV0ZGxm8faLUqIyNDixYtqnO7oqIitW/fXikpKbrkkku0bt0617pt27YpKyvLbZ/R0dFKS0urc5/l5eUqKChwewG+gj7pAAD4r3eW7NQTX200OwwAAcSrSfqBAwfkcDjcasIlKTExUVlZWbVu07VrV82YMUOffPKJ3nrrLTmdTp1xxhnavbt6Tsoj2zVmn1OnTlV0dLTrlZKScrKHBgAAAEiSftpywOwQAAQQnxvdPT09XWPGjFHfvn01ZMgQffjhh4qPj9d///vfE97n5MmTlZ+f73rt2rXLgxEDAACgOduyv1hllQ6zwwAQILyapMfFxclmsyk7O9tteXZ2tpKSkhq0j+DgYPXr10+bN2+WJNd2jdmn3W5XVFSU2wuBgf6qAADAF2zYR3dKAJ7h1SQ9JCRE/fv319y5c13LnE6n5s6dq/T09Abtw+FwaM2aNWrdurUkqUOHDkpKSnLbZ0FBgRYvXtzgfcL/bc4p1E3/W64Ok780O5RG2ZxTqIc+XXf8ggAAwK84nFQcAPCMIG9/wKRJk3TddddpwIABGjRokJ577jkVFxdr7NixkqQxY8aoTZs2mjp1qiTpkUce0emnn67OnTsrLy9PTz/9tHbs2KEbb7xRUvXI77fffrseffRRdenSRR06dND999+v5ORkXXrppd4+HPiIK6YvUl5JpdlhNNqF//pBFQ6n2WEAAAAP++e3v+o/o/srOizY7FAA+DmvJ+lXXXWV9u/frwceeEBZWVnq27evZs+e7Rr4befOnbJaf6vQP3TokMaNG6esrCzFxsaqf//++umnn9SjRw9XmbvvvlvFxcUaP3688vLyNHjwYM2ePVuhoaHePhz4CH9M0CWRoAMAEKB+3HxQ9328Vi9c3c/sUAD4OYvRDDv1FhQUKDo6Wvn5+fRP91Op935hdgge89YNaTqzcytZLMyXDs8KhO9Jp/gWmnvHOWaHAcBLAuE6dbSkqFD9/LehZocBwEc1NA/1udHdgebmmlcX69UftpkdBgAAOEmGml3dFwAvIEkHfMC/5m4yOwQAAHCSml/7VADeQJIOAAAAAICP8PrAcQAAnCgqpYDAZBiG/vktrcgAoDbUpAO+gEwEANCMzFmfrecDsKsXP+cAPIEkHfABheVV2l9YbnYYgM9hzgMgMGUVlJkdglfQJx2AJ5CkAz7inKfnmx0C4HO43wUAAM0NSTrgI4orHGaHAPicrfuLzQ4BABqBR4sATh5JOgAAAOABNHcH4Akk6QAQQA4WlWvMjCX6cs0+s0MBgGaHHB2AJzAFGwAEkCdnb9R3v+7Xd7/uNzsUAAAAnABq0gEggOQWV5gdgseNfP57Ldpy0OwwAHhQoM7cYNDeHYAHkKQDAHzaur0Fuvrln80OAwCOixQdgCeQpAMAAAAA4CNI0gEAAAAA8BEk6QAAAIAH0CUdgCeQpAMAAAAewMBxADyBJB0AAgj3hwBgHi7BADyBJB0AAAAAAB9Bkg4AAAB4AlXpADyAJB0A4Bf25pXK4eQOGAAABDaSdAAIIIGcwp7xxDyNe3OZ2WEAAAB4FUk6AMBvzNuYY3YIADzBYjE7Aq8I5AelAJoOSToAAAAAAD6CJB0AAAAAAB9Bkg4A8CsGk8EDAIAARpIOAPAro1762ewQAAAAvIYkHQACSHOoZV68LdfsEAAAALyGJB0AAAAAAB9Bkg4AAAAAgI8gSQcAAAA8oDl0OQLgfU2SpE+bNk2pqakKDQ1VWlqalixZUmfZl19+WWeddZZiY2MVGxurjIyMGuWvv/56WSwWt9eIESO8fRgAAAAAAHiV15P0d999V5MmTdKDDz6oFStWqE+fPho+fLhycnJqLb9gwQJdffXVmj9/vhYtWqSUlBQNGzZMe/bscSs3YsQI7du3z/V65513vH0oAAAAAAB4ldeT9GeffVbjxo3T2LFj1aNHD02fPl3h4eGaMWNGreXffvtt3XLLLerbt6+6deumV155RU6nU3PnznUrZ7fblZSU5HrFxsZ6+1AAAADgARazAwAAH+bVJL2iokLLly9XRkbGbx9otSojI0OLFi1q0D5KSkpUWVmpli1bui1fsGCBEhIS1LVrV9188806ePBgnfsoLy9XQUGB2wsAAhG9IQH4g0C9VhVXOPTOkp1mhwHAz3k1ST9w4IAcDocSExPdlicmJiorK6tB+7jnnnuUnJzsluiPGDFCb775pubOnasnn3xSCxcu1AUXXCCHw1HrPqZOnaro6GjXKyUl5cQPCgAAAKjD5A/XaMM+KoQAnLggswOozxNPPKFZs2ZpwYIFCg0NdS0fNWqU6+9evXqpd+/e6tSpkxYsWKChQ4fW2M/kyZM1adIk1/uCggISdQABqTk1IS2pqFJ4iE//jAFoprILytS9dZTZYQDwU16tSY+Li5PNZlN2drbb8uzsbCUlJdW77TPPPKMnnnhC33zzjXr37l1v2Y4dOyouLk6bN2+udb3dbldUVJTbCwACUaA2IT3WJyv3qMcDX+uV77eaHQoA1GCxVD8y/XnrQa3YecjkaAD4G68m6SEhIerfv7/boG9HBoFLT0+vc7unnnpKU6ZM0ezZszVgwIDjfs7u3bt18OBBtW7d2iNxAwB8222zVkqSHv1ig7mBADghgd7qxzAM7T5UolEv/azf/+cnOZ3N5REqAE/w+ujukyZN0ssvv6w33nhDGzZs0M0336zi4mKNHTtWkjRmzBhNnjzZVf7JJ5/U/fffrxkzZig1NVVZWVnKyspSUVGRJKmoqEh33XWXfv75Z23fvl1z587VJZdcos6dO2v48OHePhwAAACgXtsOFGvwk/Nd750GSTqAhvN6Z76rrrpK+/fv1wMPPKCsrCz17dtXs2fPdg0mt3PnTlmtvz0rePHFF1VRUaErrrjCbT8PPvigHnroIdlsNq1evVpvvPGG8vLylJycrGHDhmnKlCmy2+3ePhwA8Fmz12ZpQeZ+s8MAgGbvzUU73N6TogNojCYZcWfixImaOHFiresWLFjg9n779u317issLExff/21hyIDgMBx01vLzQ4BACCposrp9p6adACN4fXm7gAAAEBzRo4OoDFI0gEAAAAP2pNX6vaeJB1AY5CkAwAAAF5Ec3cAjUGSDgAAAHgRSTqAxiBJBwAAALyIadIBNAZJOuBDJr27UpuyC80OAwAAeJBBTTqARiBJh9+5+4NVZofgNR/+skdXTF9kdhgAAMCDyNEBNAZJOvzOe8t2mx2CV+WXVpodAgAA8CD6pANoDJJ0wAcZhkHTOAAAAgR90gE0Bkk64IMu+89PuvbVJSTqAAAEAH7PATRGkNkBAKhp5a48SdKjX2xQWoeW6pIYqQ5xLcwNCgAAnBBSdACNQU064MNe/WGbxv9vuc59ZoHZoQA+q6zSoetfW6KXv9tqdigAUCv6pANoDJJ0AIBf63b/bC3I3K/HvtxgdigAUKtt+4tp8g6gwUjS4Vf4gQMAAP7mj68s1mNf8CARQMOQpMOvkKMDqM9f3vlFqw6P6QAAvuSVH7aZHQIAP0GSDr9Cny4A9fls1V5dMu1Hs8MAUI/c4gp9tmqv2WEAgM8iSYdfIUUHapdbXGF2CADQIKNeWqTF23LNDsMUT3+9UaUVDrPDAODjSNLhV6hIB2oqKq/SaVPmmB0GADTIr9lFZodgmmnzt+jf8zeZHQYAH0eSDr9iNOO69EVbDqq8iqfvqGn7gWKzQwAANFBmVqHZIQDwcSTp8CvNuSb96pd/1llPzldhWaXZocDHWCxmRwAAaDgu2gDqR5IO+JGcwnL1eugbTV+4xexQ4EMs3PDVsCevVA5nM36qB8BnfbshW9e+uliVDqfZoQDwUSTp8CvNuSb9aE98tdHsEOBDrFzJazjziXn6yzsrzA4DAGr1/aYD+mZdttlhAPBR3NrBrzAFG1ATNem1+3JNltkhAECdSiqqzA4BgI8iSYdfIUUHaqJPOgD4H+odANSFJB1+xeAXDajBSpIOAH6H1oEA6kKSDr/Cz9lvGBQLklRYVql/z9tsdhg+650lO/XZqr1mhwEANfAzDqAuJOnwKzx0/k2nv32pf8/bpPIqh3KLK8wOByZ5/MsN+nglSWhdJn+4Rn955xda4QDwOQ6nUweKys0OA4APIkmHX9l9qMTsEHzKM9/8qpHP/6DTpsxRVn6Z2eHABEu3HzI7BL9QUuFQfmml2WEAgMv9n6zTgEe/1byN2Xr4s3U6UFTOA0UAkkjS4WdG/fdns0PwOZtziiRJCzJzTI4EZqA7esNc9MIPOvup+TpEqxMAPuZPry/Taz9u14BHv9Wgx+dSIQGAJB3+pbCc6UrqEmzj6+yrqhxOlVY4zA6jWdt2oFj5pZXakFVgdigAUKf9heUa/OR8fbJyj9mhADARd/VAgNh0uEYdvufif/+oPo98oyIeMpkuhIdZgKm4DjbMbbNWqqSiSm8v3qHsgjLlFldoc06h2WEBaCJNcrcybdo0paamKjQ0VGlpaVqyZEm95d9//31169ZNoaGh6tWrl7788ku39YZh6IEHHlDr1q0VFhamjIwMbdq0yZuHAPi86Qu3KPXeL/TWzzskSX/7aI2ueWWxKh1OfbB8tzZlV/+455dU6sr/LtL0hVvkdBq65pXFuuaVxYwW70Xr9xWoosqp+Rs92yWhoKxSOYUMOtQYV0xfpNR7v9BrP26TJD3zdaYuf/EnlVXS0gFoCi8uYDaKhurxwNf6+0drlfb4XJ02ZY4ynv1OW/ef+AP5z1bt1aItB2vt9+50Giour+JaCPgIi+HlESreffddjRkzRtOnT1daWpqee+45vf/++8rMzFRCQkKN8j/99JPOPvtsTZ06VRdddJFmzpypJ598UitWrFDPnj0lSU8++aSmTp2qN954Qx06dND999+vNWvWaP369QoNDT1uTAUFBYqOjlZ+fr6ioqI8fsyoqbzKoa37i/Xdr/t1ab82chqGWrYI0fIdh5QQGarOCRF1brsrt0RtYsJktVqUeu8XTRi1/5p3xxCd94+FkqTwEJtKDje1/uCmdF0xfVG9215zejvdel4XDXp8riTpruFdNeHczlqzO1+Ltx3UJX3bKD7S7t0DCCBVDqc6//0r1/uHftdD15/Z4aT363Aa6vS3L49fEHXaNvVCdZhc/W/Ys02U1u6pbgr/+V8Gq2ebaBmGIYvFom0HirV2T74u6t1axRUOhQXbZKtlcnrDMLT1QLE6tGohq9WiX3YeUkx4iDrEtWjS4wJ82YSZK/TF6n1mh+G3gqwWLfl7hsa/uUwX9mqti3q31r78MrWODtU/v92kOeuz9Op1A9W+VbhiwkO0IDNH17+2VBf2StKXa7Jc+7l6UIrySirVLSlKE8/rrOtmLNEPmw+41n82cbB6tY2WJL303Rb9Z8EWDe4cp2ev7KuQIKsqHU45nIZCg21N/m8A+LOG5qFeT9LT0tI0cOBA/fvf/5YkOZ1OpaSk6C9/+YvuvffeGuWvuuoqFRcX6/PPP3ctO/3009W3b19Nnz5dhmEoOTlZd9xxh+68805JUn5+vhITE/X6669r1KhRx43JX5J0p9OQoeobv+r/Vi83ZPz2t3HM+2PLH7PNsesdTkMFZZWKsAcpq6BM7VqGa2duiSySosOCNWd9tt5avEMPXnSqCssr9e2GHK3cmaeL+rRWZZWhGYdro87s3ErtW7XQzMU7T+qYrz29vSac21lhITat2HFIY19f6lq3+bEL3JId+J5PJ56pi//9oyRp4V3nKCo0WP+3YrdKKxxKjA7VJX2TZQ+yqdLhVF5JpezBVuUVV+rrdVlq1ypcw3okKqewXD9uPqBFWw6qbWy4UlqGKaewXK98v00jeibqvpE99NXaffrru6sUF2HX86P6uhKnr9bu04qdeerVJkoP/u5UVTkMLdy0X9v2F+uiPq3VKT5CReVVuvw/Pykzu1BndYnTG2MH6eJpP2jtngLdNrSLbs/o4krcJOmLWwcrr6RSY19fqg6tWujdP5+u06bMcc1v+/DFp2rIKfH6/Ys/qbTCocV/H6pnv/lVr/+0XZL0x7R2x/1etI0N04D2sW5TqW19/EJZrRYVllWqRUiQth4o1u5DJerZJlpxEXbtyy9V+tR5nv0fiBqOXBM9ITk6VA7DUHZBuX7XJ1k2S/U1eWBqS3VOiNDSbbnq2SZa6Z1aKTTYptziCu05VKoKh1Od4yMUHR4sSSosq1RBWZVsFoviI+2yWS3am1eqO95bpb+P7K5Tk6O08Nf9uv61pXr2yj66sFdr/XfhVv3z21/12GU9NTqtvTbsK9CaPfnq0zZGocFWhYcEKT7SrpKKKm3JKVaV06luSVGqqHIqp7BMnRMitDe/TPsLy9WjdZRCgqzal1+qyNBgRdiDVFJRpaz8MpVUONSuVbiiQqtjLSqv0u5DJYqPsKtlixBJUl5JpYorqtSyRYjCQ4K0L79Uuw+VqlebaIUG2/TTlgP6dOVedYqP0LXp7RUabNOevFJF2INks1p0oLBcrSJCFBkarLJKh4rLq9Qqwq6ySoeWbs9VTFiIeraJUoXDqV925ikyNEg9WkfpQFGFyqscahsbrrJKh5bvOCSb1aI+bWMUFmJTlcOpHbkligoNVnRYsIJtFlU5DeWXVspqsahlixDllVToQFGF2rcKV7DN6jq+gtIqdWsd6Tpuqbpv8db9ReoQ10IJUaEqq3Qo2GaVzWrR/sJyrdubr7QOrWSxSBUOpwpKKxVisyo+0i6LxaLNOUX624drFB9lV3yEXRaLdGGv1kqJDdfL32/VrtwStYqwK9hmUeeECBWWVenprzMbfV7+a1Rf7csv0xNfbXQtG3JKvG7L6KLf/+cn17IWITYN6tBS8ZF2vbdst1vZ0WntdN/Ha5VTWK7wEJtWPzhMQTarPlu1V6//tF1jz0xVaqsWOjU5yu36isDx2GU9dVm/Njr/2e+0J69UknTb0C4KCbKqa2KkeiRHafmOQ+rZJlod4lqovMqh0gqHWtiDZLVYVFrpUFFZlVbuOqR9+WXq1y5Wn6/aq1d+2Kbrz0jV9oPFWpC5X5f0TdYt53TWtPmb5TQMLfx1v8af1VHjh3RUYVmVJry9Qou35eqRS07VmPRUVTqcqqiqnuouOqz6u22xWGQYhiocTuWXVio6LFj2oOqHDYVllcorqb4vjg4LltVqcbU0PPJsNqugTIYhxUfaZbVYVOV0qspRfW8dGmRVkM0qwzBUXuWUVD1mUEWVU6HBVlks1TupcjhVXOFQiM2qsJDqz3Y4DVU6nK5/02CbVVVOp4KsVrcHw1UOpyoOH1ewzSqrxf2hsc1qUbDNIsOQnIfv+Y9mtVhktcgVi9mcTkNVTkNWi2ocy4mw1vIQ3Zf4RJJeUVGh8PBwffDBB7r00ktdy6+77jrl5eXpk08+qbFNu3btNGnSJN1+++2uZQ8++KA+/vhjrVq1Slu3blWnTp30yy+/qG/fvq4yQ4YMUd++ffWvf/2rxj7Ly8tVXv5bk9CCggKlpKT4fJJ+0Qvfu2p2gEAxMDVW6/YWuGr3cXz2IKvrxz7IalFqXAvXqP4ITG1iwrQ3v1RH/0J3S4pUQWml9po83eLR56MkBdssqnS430p0iGshe5BVv2YXuh5o2YOsMozqhPSIli1CdKikQoYhxYYHq2N8hJbv+G1awZSWYYoND9Hq3flu+7dYpIRIu3IKy2UY1Q9T9heWq/RwU934SLscTkO5h0fzT44Odft3O7qFUZDVorgIu4rLq9wGJ7VapKN7AUWGBqmovEqGUf0QO7VVuFbvyXf9P7JYquOoclQ//C4s+21fIUHVN+l1/XsBzUnb2DBlF5SZ8j0IsVkVbLOorMrp1s0vNNgqe5DNbapOq0UKDba53a9YLNLxMqcjD/iOLWezWhRktbhdP4/EdPR1sTZWS/X2FovFdS05WZbDSfHJpLQn83/w2EpIT0jv2ErvjD/dczv0goYm6UHeDOLAgQNyOBxKTEx0W56YmKiNGzfWuk1WVlat5bOyslzrjyyrq8yxpk6dqocffviEjgGAZzGvd+OVVzllD7KqVYsQ7c0vI0FvBo7URAXbLIoOC9aBogptzPpt0CgzE71jbzCPxNEixKawEJsOFFVo24Fi1/qo0CAVlFW5bXfkpvRIEh0THqxDJZVavuOQLBbpgp5JWrr9kHbllmpXbqnb5x1JsLMLfnv4fqS1Q3ykXSXlVdp/eKyG6LDq2vZjH2yUVDiUGGWXYUg5heXKKqhebw+qjqu6Bsr9uI8k3eEh1Tfyqw4/OIgJD1Z4sE1788u04+BvrS4sFikxMlRZBWVuN9WVDkMWS/X0iQwFguZo96HS4xfykgqHU7XVEZRVOlVWWf09PfJQzWmoRoWCYVQ/2DOkOsfyqeva7HAatW5zvARdqr5WOB1HNZH1AMOQHN5tUI2T4NUk3VdMnjxZkyZNcr0/UpPu696+8XQ5nUd+zC2uCZGP/Lhbjnr6daTM0a1Ejl52pLx05G+5mvtsO1CsgrIqhdis2rCvQGEhNgVZLfo1u1A7c0vUJyVGs9dmacWOQ0rvFKc1e/KUXVAue5BVVw5I0fp9BW41HyejTUyYSiqqFBseounX9leHuBY6+6n52mdyzREa7ventdGIU5M0/n/LJUn/vKqPLu7TRpK0Yuch7ThYos4JEerRuro5qtUihYcEKaewTIMeq+4Hf2nfZP28NVfXprdXsM2iVbvzde+Ibrrz/VVavC1XIUFWzbhuoD5dtcet2aXVIn1/z3kqKqvSQ5+u06KtB3XTkE4a0D5W7y7bpTnrszUwNVYvXH2akqJD3cY4WHDnOTrnmQWSpOGnJuq/1w7QN+uyXMfRo3WUrujfVo98vl6S1L99rK4akKK7/2+12sSE6bHLeurO91fpQFF10vHhLWe4NRU91tgzU/XNumxXMnbEuoeH69QHv5Yk3Teyu87oFCdJ6hjfQqHBNm3KLtTuQ6UKC7Fp1Es/n9j/JJiqd9toVVQ5ldE9UeF2m3IKyhVss6h32xglH74GGobUKSFCbWLCJEk7DhZr3d4ChYfYNCC1pSLsQap0OFVYVqVduSVqHR2q+Ei7Hv5svV7/abuuHNBWD118qibO/EXzNuaoTUyY5t4xRAsyc3TTWyt0ad9kPXF5b0lSaYVDH/2yR5+v3qubhnTSqW2iFRcRotIKhyyW6maTuw+VKjosWC1bhCgzq1ALf92v83skKj7CrrIqh4KsVtdYFbnFFVqzJ19llQ71TYlRYlSoyqscyikoP1wDHqpgm0W5xRXal1+mli1ClBBp14LM/dq8v0hDTolX99ZRKiir1Os/bpckXTUwRcE2qyySYluEKKegTLvzStWuZbisFotW7jqk+IhQ9WwTpfIqp777db+CbBYN7hyv0gqH1u3NV/u4FsorqdD0hVt11YAUnd6xpWxWi/bklepQcaUsFumUxEhZLVJxuUNlVQ7Zg6yKsAepvMqp3YdKFRMerLgIu1bsrG6S2+tw011Jyi6oTtKPPFiJi7QrKjRYucUVKjjcpLbq8OBcocE2JUTa9cuu6nFZWkWEKDTIpl9zCvXj5oOa8vl63Ta0i24b2kUHiyu042CxFm/LdTVnH9Shpf6acYreX75LH674baquP5/dUalxLTT5wzXHPQ+vHtRO7yxx74rz8MWn6tUftjWoi8d16e31xqIdbssmnNtJZ3WJ1+y1Wdqwr0C3nNtZQ06pfn/TW8uPu08EhjYxYa7ftuvS2yujR6JW787Xed0SFBdh16bsQrWNDVdSdKhKKqrkNKSwYJs2ZhVo96FSzVy8U4u2HtTZp8Rrx8Fi18Ovi/sk60BRuX7aclAhQVad3yNRWfllCrFZtWjrQXVLitQLV/dTaLBNH/2yR8E2q6LDgpXRvfpzSysd2pNXKqdhKCYsRPYgq6LCglVUXqWC0kqVVDiUFB2q6LBgVVQ5lVdaofLK6ofkQTarHE5DTsNQbHiIgqwW5ZVWyiLJZrMo2Fo9FndZpUOllQ4FWS0KtwdVN6uvciokyKqSCocqqpyyB1sVZK2+thSXV6m00qGQIKtsFotsNoush+/PKx2GbFaLqg73/3cYhpxGdZP60GCba2yAoxmGVOUwVOV0uvIEtzzicDfZ6mOpbg5/sk6qcblFrub8xuHjOxlBNt9u6t4YzaK5+7H8pU96oHnsi/Xam1emF67up/X7CpQYFdrgAcgMw9Dq3fn6eOUevXb4pg31S2kZVqMG6oijB8mqTWKUXZef1lY/bjmoVbvyJEkvjxmg83tUt2CpqHLq3WW7FB8Roh0HS3TjWR1rHUgL0pTP1+vVH7a5LesQ10Lz7zzH9X7tnnwdKCrXOV1rDqZ5PAymeHI2ThmhbvfPdr2/9bzOen7eZv332v4a1iNRTqO6iaHTaejmt5crOSZMD/7uVLd9GIahdXurH3B2iv9tEMyySoeKyqsUF8FAi2haRwY9zC+tVHiITcGHpx40DEMFZVWKDvut73ylw+la3xBHagJP9Jq/J69UZz7BeBonIq1DSy3elitJeu/P6Wpht+nprzP1pzM7aMyM6pmT3r4xTV+vy1JxuUO7D5XozRsG6cmvMl1jCB3x2vUDlRQdqm5JkbJYLLrgX99rw77q+4IrB7TVk5f31tfrstUnJVpJUaHafahUUaHB+ue3v2rxtlx9cFO6WtibRV0f4FE+0Sddqh44btCgQXrhhRckVQ8c165dO02cOLHOgeNKSkr02WefuZadccYZ6t27t9vAcXfeeafuuOMOSdUHm5CQEHADx6GmJ2dv1IsLtpgdhs8ackq8Fv66XyE2qzIfHaEBj36rg8UV2vzYBfpg+e7qhxzXD5LNatEp91UPwhcVGqTRp7fX5ae1UeeESJOPIPA8/uUGvfTdVklS5qMjtCm7SKcmR3lswBaS9BN329Au+uv5p+j9Zbv00KfrNOP6gUrr2MrssICAtr+wXAMf+9bsMPzCsd1aMh8dof5Tqv/tVtx/vkKCGv5wpazSodBgmxZk5qi43KGRvVvXWs7pNHx+4C3An/lEn3RJmjRpkq677joNGDBAgwYN0nPPPafi4mKNHTtWkjRmzBi1adNGU6dOlSTddtttGjJkiP7xj39o5MiRmjVrlpYtW6aXXnpJUnUT7dtvv12PPvqounTp4pqCLTk52a22HoHJ5iMjUfqq83sk6o0/DXK9X37/+a6/Rw1qp1GD2rneL/17hmYu3qkrB7ZV6+iwJo2zOQk66mbHHmRTzzbRJkYDSfp9vzZ69LKeCg+p/gn8w4AU/WGA73eBAgKBPbjhiWVzd+8F3XXt6e01Z322BneJkz3IpjUPDZPU+JG5j0yVdrwWWyTogG/wepJ+1VVXaf/+/XrggQeUlZWlvn37avbs2a6B33bu3Cmr9bcL9hlnnKGZM2fqvvvu09/+9jd16dJFH3/8sWuOdEm6++67VVxcrPHjxysvL0+DBw/W7NmzGzRHOvwbPx71+8OAtg0uGx9p120ZXbwYDSR5vTng0X3/0DB3j+jmStABNK2QRjStb65uG9pF8zNzNGpgikKCrG613r4ybRYA7/J6c3dfRHN3//X83E16ds6vZofhk87pGq/Xxw46fkE0qYKySv3hxUUadmqi7hjW1eP7zyko06DH53p8v4Fs/SPDSdIBkxiGwVzpx7H9iZFmhwDAS3ymuTvgSQxO5m7q73vJMKSRvVsrKpSvsy+KCg3W138922v7T4gK1U1DOmn6QsZqOFaHuBa6/6Lu+tPryyRJ/xrVV0FWKwk6YCJqggHg+LhTgV+x8uPu5uqj+pij+brl3E5atOWAa95mVDsygj61UgB81bizOujDFXt0sLjC7FAA+BA6BsGvUJH+m1OT6aqBalGhwZp+bX+zw/AZnRMitOrBYWaHAQDHtXJXnpbdl6EPbzlDklxTnQJo3qhJh19pdgMo1OHmczrp+jNSzQ4DPoSuIL/pmhjpNg80AN/SLSlSG7MKzQ7DNKcmR2nd3uo5yROjQmWxWHRau1gtvy9DseEhJkcHwBdQkw6/Mmog0yRJ0j0juikxitkM8JsgK5fzI/42srvZIQCox/RrmnfLny9uPUt/TKvurjblkt9mL2oVYWcWGwCSSNLhZ2LCQ/T93eeaHYZpYsKD9RLNmlELG+M1uLSJCTM7BAD1SI1r0eynYnv8sl7a/sRIxbag5hxATTR3h99JaRludgim+eX+8xkZF7Wy2TgvJGmOF0fSBwAAaArN+zEm4GdI0FGXoGbaRHJgaqzr77axYeqSGGliNAAAACePJB0AAkBzHTjOHmRT29jq5u3DT00yORoADdY8L1kA0CA0dweAANCc+6S/9+d0zc/M0eWntTU7FAAN1ByfK8ZH2vXKmAFmhwHAD5CkA35i0vmnmB0CfFhzHhE4OSZMo9Pamx0GgEawNMOq9E8nnqnW0QxsCeD4aO4O+IFuSZG6dWgXs8OAj2PkfwDwXdZm3OIJQOOQpANAgBhGn2wAfqI55qvN8JABnCCSdACA3zJkmB0CgBPQLBPWZnnQAE4ESTrgg47tXjzxvM7mBAIAgBcwpSgA1I2B4wAf9OnEwZq7IUfXprdXaaVDbWIYaAYAEDiaY4reHAfLA3BiSNIBH9S+Vbhuy2CgOABAgCJfBYA60dwd8EHBNr6aAIDA1RxzdFr4A2goMgHAB5GkAw1jMG4c4JeaW5/0Ae1j1apFiNlhAPATZAKAD7IdO3IcgFqRpAP+qZnl6Hr/pvRm92ACwIkjSQd8zMoHzjc7BAAAvKo5pas3DelEgg6gURg4DvAxMeE0hwMABDZngLeCiQ0PVlqHVnryit6KDgs2OxwAfoYkHfAh943sbnYIAAB4nRHgfVWW/j1DNquFGnQAJ4QkHfAhN57V0ewQAL9iKLBv9IFAFeA5uoIYABbASeAKAgAAgCYVZKOGGQDqQpIOAACAJhUSxC0oANSFKyQAAACaVKAPHAcAJ4MkHQAAAE2qW1Kk2SEAgM8iSQcA+K1AH3wKCFT/+EMfDe2WYHYYAOCTSNIBAADQpBKiQvX473uZHQYA+CSSdAAAADQ5xncHgNp5LUnPzc3V6NGjFRUVpZiYGN1www0qKiqqt/xf/vIXde3aVWFhYWrXrp1uvfVW5efnu5WzWCw1XrNmzfLWYQAAAAAA0GSCvLXj0aNHa9++fZozZ44qKys1duxYjR8/XjNnzqy1/N69e7V3714988wz6tGjh3bs2KGbbrpJe/fu1QcffOBW9rXXXtOIESNc72NiYrx1GAAAAPAGqtIBoFZeSdI3bNig2bNna+nSpRowYIAk6YUXXtCFF16oZ555RsnJyTW26dmzp/7v//7P9b5Tp0567LHHdM0116iqqkpBQb+FGhMTo6SkJG+EDgDwA/df1EPT5m/Wo5f2NDsUACfIQpYOALXySnP3RYsWKSYmxpWgS1JGRoasVqsWL17c4P3k5+crKirKLUGXpAkTJiguLk6DBg3SjBkzZBxneN/y8nIVFBS4vQAgEP2hf1uzQ2gSNwzuoOX3ZahLItM4AQCAwOKVmvSsrCwlJLhPqxEUFKSWLVsqKyurQfs4cOCApkyZovHjx7stf+SRR3TeeecpPDxc33zzjW655RYVFRXp1ltvrXNfU6dO1cMPP9z4AwEAP/P473uppNKhL1bvMzsUr7NYqIUD/BlfYQCoXaNq0u+9995aB247+rVx48aTDqqgoEAjR45Ujx499NBDD7mtu//++3XmmWeqX79+uueee3T33Xfr6aefrnd/kydPVn5+vuu1a9euk44RAHxRsM2qfikxZocBAACAE9SomvQ77rhD119/fb1lOnbsqKSkJOXk5Lgtr6qqUm5u7nH7khcWFmrEiBGKjIzURx99pODg4HrLp6WlacqUKSovL5fdbq+1jN1ur3MdAAAAmh4V6QBQu0Yl6fHx8YqPjz9uufT0dOXl5Wn58uXq37+/JGnevHlyOp1KS0urc7uCggINHz5cdrtdn376qUJDQ4/7WStXrlRsbCxJOAAcRjNwAP6AaxUA1M4rfdK7d++uESNGaNy4cZo+fboqKys1ceJEjRo1yjWy+549ezR06FC9+eabGjRokAoKCjRs2DCVlJTorbfechvgLT4+XjabTZ999pmys7N1+umnKzQ0VHPmzNHjjz+uO++80xuHAQAAAABAk/LaPOlvv/22Jk6cqKFDh8pqteryyy/X888/71pfWVmpzMxMlZSUSJJWrFjhGvm9c+fObvvatm2bUlNTFRwcrGnTpumvf/2rDMNQ586d9eyzz2rcuHHeOgwAAAB4AfXoAFA7ryXpLVu21MyZM+tcn5qa6jZ12jnnnHPcqdRGjBihESNGeCxGAAhE3PgCAAD4L6/Mkw4AMA/dPAH4A65VAFA7knT4pdfGDjQ7BAAm+eavZ5sdAgAAgNeQpMMvnds1Qad3bGl2GACamMUinZIYaXYYADzAQuccAKgVSTr81nGGMACaLW57AfgFLlYAUCuSdACA3+CeHgAABDqSdPitQBtwJsLutckWAADwOYH2Ow4AnkKSDviIL24dbHYIAAAAAExGkg74iPatWpgdAuDzLFS9AQEjUL/N5/dINDsEAH6O9rWAD+gYT4IOz2FMRQAwx0e3nKE+bWPMDgOAnyNJB3zAxxPONDsEwC8Eas0b0BwFWsuYNjFh6tcu1uwwAAQAmrvDbwXK/KpRoUGKCg02OwzAL4w7u6PZIQDwkMD4FQcAz6MmHQACjBGg7d3/7+Z0mpECAICAR5IOmCxA8ynA4/q3b2l2CAA8KMBauwOAx9DcHTDZq9cNNDsEAAAAAD6CJB0w0fNX99OgDtQOwrNa2G1mhwAAxxUoY8sAgKeRpMNvBUIzOWsAHAN8z6X92ujcrvFmhwEA9QqE33EA8AaSdPitQBgci1oEeIM9yKbXxg4yOwwAAACcAAaOAwD4tFvO6aTBnePMDgOAhwXCw3YA8AaSdPgtmskBzcPdI7qZHQIAAECTobk7AAAAmpzBJKQAUCuSdMBEtAYAAAAAcDSSdAAAADS5QBs81cpdNQAP4XICmCiwbk8AAGi4sBCbLuvXxuwwPCbQHjoAMA9JOvxWIDQVD4RjAADgRP3zqr5mh+AxVn7TAXgISToAAADQSN1bR7m9bxVhNykSAIGGJB0wFY/d4T1pHVqaHQIABKw3xg7UlEt7ut4/e2UfE6MBEEhI0gEgQM0af7rZIZy00zvyoAGAb7JYLPpD/7bq3jpKf0xrp/atWpgdEoAAEWR2AMCJMgJgelX6pMObLAFwgt05rKvZIQBArSwWKTTYpq9uO8vsUAAEGGrSAQAAAADwESTpgIn8v54TAAAAgCeRpAMmCoTmyAAAAAA8x2tJem5urkaPHq2oqCjFxMTohhtuUFFRUb3bnHPOObJYLG6vm266ya3Mzp07NXLkSIWHhyshIUF33XWXqqqqvHUYgFeRogP1i2NKIwAA0Mx4beC40aNHa9++fZozZ44qKys1duxYjR8/XjNnzqx3u3HjxumRRx5xvQ8PD3f97XA4NHLkSCUlJemnn37Svn37NGbMGAUHB+vxxx/31qEAXhMdHmx2CIBPS41jtGQAANC8eCVJ37Bhg2bPnq2lS5dqwIABkqQXXnhBF154oZ555hklJyfXuW14eLiSkpJqXffNN99o/fr1+vbbb5WYmKi+fftqypQpuueee/TQQw8pJCTEG4cDeMWfz+6oAe1jzQ4D8Fkr7j/f7BAAAACanFeauy9atEgxMTGuBF2SMjIyZLVatXjx4nq3ffvttxUXF6eePXtq8uTJKikpcdtvr169lJiY6Fo2fPhwFRQUaN26dXXus7y8XAUFBW4vwEzndUvQ5Au70ycdqENSVKhatuDBKwAAaH68UpOelZWlhIQE9w8KClLLli2VlZVV53Z//OMf1b59eyUnJ2v16tW65557lJmZqQ8//NC136MTdEmu9/Xtd+rUqXr44YdP9HDgo/x5nnTDn4MHmgDPr4Dmp2tipDKzC80Oo8G4TAHwlkbVpN977701BnY79rVx48YTDmb8+PEaPny4evXqpdGjR+vNN9/URx99pC1btpzwPiVp8uTJys/Pd7127dp1UvsDAHgXN79A8xMf6V8DRfK4HYC3NKom/Y477tD1119fb5mOHTsqKSlJOTk5bsurqqqUm5tbZ3/z2qSlpUmSNm/erE6dOikpKUlLlixxK5OdnS1J9e7XbrfLbvevCz8AAAAAoPlpVJIeHx+v+Pj445ZLT09XXl6eli9frv79+0uS5s2bJ6fT6Uq8G2LlypWSpNatW7v2+9hjjyknJ8fVnH7OnDmKiopSjx49GnMoAAAAAAD4HK8MHNe9e3eNGDFC48aN05IlS/Tjjz9q4sSJGjVqlGtk9z179qhbt26umvEtW7ZoypQpWr58ubZv365PP/1UY8aM0dlnn63evXtLkoYNG6YePXro2muv1apVq/T111/rvvvu04QJE6gph1+hiRwAAP6N4WUAeItXknSpepT2bt26aejQobrwwgs1ePBgvfTSS671lZWVyszMdI3eHhISom+//VbDhg1Tt27ddMcdd+jyyy/XZ5995trGZrPp888/l81mU3p6uq655hqNGTPGbV51NB/GUanuWV3iTIwEAAAAADzDK6O7S1LLli01c+bMOtenpqa6jXCdkpKihQsXHne/7du315dffumRGBE4/ndDmlLv/cLsMAB4CNMTAs3HpX2T9fHKvbrlnE76YfMBs8MBANN5LUkHUDeayAEAUO2fV/XVQxefqpjwELNDaRSDzmsAvMRrzd0BAOZLjg41OwQAqJfFYvGbBL176yizQwDQDJCkw29dl54qSTqjUytzAwF82Nw7zpGVluMA4BEvXN3P7BAANAM0d4ffuqBXa82/8xy1jQ0zOxTAZ4WF2JTaqoW2Hig2OxQA8HtxEf5R4w/Av1GTDr/WIa6Fgm3+dxrTiw34zZ/P7lhjGePGAQCA5sr/shsAQKPccm5ns0Oo1+QLu6tNjHuLmL9mnGJSNADQQDxxB+AlJOkAEOCu6N/W7BAa7XI/jBlA4LOIZj4AvI8kHTCBwRxsAAD4NX7JAXgLSToAAAB8lk9NJUlFOoAmQJIOAAAANATV5wCaAEk6AAAAAAA+giQdAAAAPiWje4Lrb09VXrcIsZ38TmjuDqAJkKQjYBw7hRMAAPBXv2XDTg8Ntjq4S5x6tYn2yL4AwJtI0hEwnr+6n9khNJjFwqN4AAAawumhqnSLLPrnVX09szNJTNQCwFtI0hEwgqxNm/iSZwOec9XAFLNDAOAD4iJCJEnDeiS6ljk9laVL6pwQodfGDnRbdveIrg3ent9+AE2BJB04QQvvPNfsEICAccs5nWRr4gdtAHzPN38dojf/NEhX9G/rWhYTHuyRfU88r7Mk6ZTESLflt5zT+YT2ZzDUOwAvIUlHwGjqp9vtWoUrIdJ+Qtv2TI7ycDSAfwuyWXVGp1ZmhwHAZC1bhOjsU+JlPeqhXXJMmB76XY+T3nfPw/3RT2YMGx4lAmgKJOnACTjS/z28kSPFfnnrWfprxim6dWgXb4QF1OnuEV2V2irc7DAA4IRcf2YHr+7/ruENb/IOAN5Gko6AcfQALkNOiffqZ/VoXd1ULjwkqHHbJUfptowuCg32wDQwQCPcck5nLbjLt7toMAgTgKbSOjrU7X1qqxaN3gfXLADeQpKOgDTj+oHHLwQAAJqll8cMUKf4Fnrp2v5mhwIANZCkI2Ac3Se9qfqMMcor4Dm92jJ/MYDf/P3C7mrVIkQPXXyqx/fds0205t5xjoadmiRJ6tcuptH74B4AgLc0rq0uEMAi7UEqLK9q1DY0dQM859bzuqhFiE0ZR029BKD5Gnd2R914VgdZmiAbTj6BweS4BwDgLdSkI2D9a1TfRpVvzG8tP8zwV1/ffrbO6hJndhi1CguxaeJ5XdQtidkPAFRrigS9MXwtHgCBiSQdAePoxNlikS7p26aR2zcs8+7dNlqd4iMatW/AV3RNitR53RLMDgMA/B7P6wF4C83dgUb6ZMKZrifpPFAHAKD54GcfQFOgJh0B42QT5oY+EaepGwAAAABvIUkHDrOSfAMAgAZqaDc5AGgsknTgsFYRIcct84f+bZsgEgAAAADNFUk6IGlw57gGjdj+9B/6eD8YwMuo/AEAAPBdJOkISPQbB8x3Ttd4s0MAAADwOyTpANDMnN6x1XHLdEmI0Ne3n33Cn3HX8K4af3bHE94eAHwRdQAAmoLXkvTc3FyNHj1aUVFRiomJ0Q033KCioqI6y2/fvl0Wi6XW1/vvv+8qV9v6WbNmeesw0ExcNTDF7BCAJtMjOUrnHqeWu0tihDonRJzwZ1zcJ1nBNp4DA/B/g1Jb6or+bfXmnwa5LafrEABv8dod1OjRo7Vu3TrNmTNHn3/+ub777juNHz++zvIpKSnat2+f2+vhhx9WRESELrjgAreyr732mlu5Sy+91FuHgWbg69vP1kW9W5/QtjxRh7964Y+n6ZK+yR7ZV3RYsEf2AwC+KCI0SM/8oY/OPiVeFmZKB9AEgryx0w0bNmj27NlaunSpBgwYIEl64YUXdOGFF+qZZ55RcnLNG0ObzaakpCS3ZR999JGuvPJKRUS41+bExMTUKAuc6A9n16TI45Zp2SJEz4/qd0L7B3xRhD1I/xrVT5+s3FtnmYZ8o0KCrHrxmtP0x5cXey44AACAZswrNemLFi1STEyMK0GXpIyMDFmtVi1e3LAbueXLl2vlypW64YYbaqybMGGC4uLiNGjQIM2YMeO481SWl5eroKDA7YXAY6jmefDk5b1OantJSu/YSsvvy9DgLnEnHBvgb+Ij7Ce9D+qbAAAAGs8rSXpWVpYSEhLclgUFBally5bKyspq0D5effVVde/eXWeccYbb8kceeUTvvfee5syZo8svv1y33HKLXnjhhXr3NXXqVEVHR7teKSn0P24uLunb5qS2j4+0a/q1/escLT7ISp9bBKZJ53dtcNm6WrHQXROAr5h8QTeP7IdubgCaQqMyjHvvvbfOwd2OvDZu3HjSQZWWlmrmzJm11qLff//9OvPMM9WvXz/dc889uvvuu/X000/Xu7/JkycrPz/f9dq1a9dJx4jm4bJ+bertb/vUFb3VOjq0CSMCmkZ0eHCDbka5XwXgD04mueY6B6CpNapP+h133KHrr7++3jIdO3ZUUlKScnJy3JZXVVUpNze3QX3JP/jgA5WUlGjMmDHHLZuWlqYpU6aovLxcdnvtzTPtdnud64D6RIXW/xU5JTFSiyYPVeq9X7iWnd8jUXPWZ3s7NKBJXdgrSV+uqdkSitpyAM1JaLBNQ7slqLiiSm1jw8wOB0CAalSSHh8fr/j4+qftkaT09HTl5eVp+fLl6t+/vyRp3rx5cjqdSktLO+72r776qi6++OIGfdbKlSsVGxtLEg6Pjrj61OW99fW6LP1pcIdGb/vg73poV26JNmYVeiwewGxdE6NqTdLrQ+0TAF9xMvcIxz6MfPX6gScXDAAch1c61Hbv3l0jRozQuHHjtGTJEv3444+aOHGiRo0a5RrZfc+ePerWrZuWLFnitu3mzZv13Xff6cYbb6yx388++0yvvPKK1q5dq82bN+vFF1/U448/rr/85S/eOAw0Y1cOTNGr1w9UeEjjJ0BoGxuu2bef7YWogKbx4O96SFKdYzEcjUQcQFMZ1iNRkvu0j4lRDaukCbfbvBITAHiD10a9evvtt9WtWzcNHTpUF154oQYPHqyXXnrJtb6yslKZmZkqKSlx227GjBlq27athg0bVmOfwcHBmjZtmtLT09W3b1/997//1bPPPqsHH3zQW4cBP+I8zij/R4w8ak70u4b/NjhWAzev1WX9Tm6AOsCXjD2zZguSumY/AICm8o8r++jZK/to9u1nuZa9Oz5d8+4YUuc2Z3WJ03ndEnRF/7Yn/Lk8jATQ1LwyT7oktWzZUjNnzqxzfWpqaq1Tpz3++ON6/PHHa91mxIgRGjFihMdiRGAxcwC3gakt9dEve0z7fMAswTYrCTyAJhEZGqzfn9ZWlQ6na1lClL1Gq7epv++lyR+ukSRdl56qjMM18ADgL7yWpANNLSEqVO+MO12RxxnsrS4tTqBp+xFMyYJA8fzV/Wpdfl63BH2xep825RS5liVFherff+yniqNumI8W2yKkxrK/nNdZQ06J1xXTF3kmYADNTrDNqm8nna0qp1Frt7RebaI9+nk8hgTQ1JjkGQElvVMr9TzOj/P53Wt/ov781f3UJSFC/xl9mkdj6t8+VoM7x+mpy3t7dL+ANwyro8YpJMiqb/7621gL3ZIi9fPfhmpAass699UpPqLGsrO6xNe7DQA0ROeESHVLivLoPs/rluDR/QHAiSJJR7Py9o1puqRvcq3ruiZFas6kIbqwV+ta15+oCHuQ3roxTVcOTPHofgFP+NuF3Rpcts6B5OqpZmKKIgD+4p9X9tUt53SqsZzGcgCaGkk6mpUzO8c1aMRqoLkYf3Ynt0GX6vp6tI6qO9murynoY5f1qnX5kFOqp9i8tI6HZgDQGG/+adBJ7yM6PFjXn5F68sEAwEmiTzrgZUlR5g1oBzSEzVr3g6vv7jpX5VUORYcHuy3vkhjZoH0POSVeG6eMULf7Z7st//cf++m7Xw/o3G7xjQ8YAI5x9im1X0vswY2rj6rtoWOwjTotAE2LJB3NWueEmn1mPeX1sQP1/vLduveChjcnBsx27KQb7VqFu73/dOKZen/Zbk06/xTXsuN9j0KDbYqwB6movErdW1cn95GhwW7TIQKAJ92e0UWZWYU6s1Nco7Y7+hp41/Cu+mD5bv19ZHcPRwcA9SNJR0ALOerp92tjB7r+/njCmVqzJ7/OQbIa6+I+yZo2f7PO6NTKteycrgk6pyuD0MD3pcSGq29KjFrYbbIH1V9j1LttjHq3jXFblhgVqq9vP1vBNovO+8fCWrdbdl+GKhxORYYG17oeADwlPtKu2zNOOX7BWhx9DRx7ZqomnNvZU2EBQINZjNomKw9wBQUFio6OVn5+vqKiPDsyKHzPoeIKOQxDcRF2r36OYRj0d4ffOvJTcDLnsMNpqNPfvpQkrX9keK1TIwGAtyzdnqui8iqde5IPyP+zYLNsFov+PKTmIHIAcDIamoeSpJOkA4DH7MkrlcNh1GgmDwAA0Nw1NA+lmgMA4DFtYphyDQAA4GQwXCUAAAAAAD6CJB0AAAAAAB9Bkg4AAAAAgI8gSQcAAAAAwEeQpAMAAAAA4CNI0gEAAAAA8BEk6QAAAAAA+AiSdAAAAAAAfARJOgAAAAAAPoIkHQAAAAAAHxFkdgBmMAxDklRQUGByJAAAAACA5uBI/nkkH61Ls0zSCwsLJUkpKSkmRwIAAAAAaE4KCwsVHR1d53qLcbw0PgA5nU7t3btXkZGRslgsZodTp4KCAqWkpGjXrl2KiooyOxzA6zjn0ZxwvqO54ZxHc8M5j2MZhqHCwkIlJyfLaq2753mzrEm3Wq1q27at2WE0WFRUFF9sNCuc82hOON/R3HDOo7nhnMfR6qtBP4KB4wAAAAAA8BEk6QAAAAAA+AiSdB9mt9v14IMPym63mx0K0CQ459GccL6jueGcR3PDOY8T1SwHjgMAAAAAwBdRkw4AAAAAgI8gSQcAAAAAwEeQpAMAAAAA4CNI0gEAAAAA8BEk6QAAAAAA+AiSdB82bdo0paamKjQ0VGlpaVqyZInZIQFuvvvuO/3ud79TcnKyLBaLPv74Y7f1hmHogQceUOvWrRUWFqaMjAxt2rTJrUxubq5Gjx6tqKgoxcTE6IYbblBRUZFbmdWrV+uss85SaGioUlJS9NRTT9WI5f3331e3bt0UGhqqXr166csvv/T48QJTp07VwIEDFRkZqYSEBF166aXKzMx0K1NWVqYJEyaoVatWioiI0OWXX67s7Gy3Mjt37tTIkSMVHh6uhIQE3XXXXaqqqnIrs2DBAp122mmy2+3q3LmzXn/99Rrx8DsBb3rxxRfVu3dvRUVFKSoqSunp6frqq69c6znXEeieeOIJWSwW3X777a5lnPdoEgZ80qxZs4yQkBBjxowZxrp164xx48YZMTExRnZ2ttmhAS5ffvml8fe//9348MMPDUnGRx995Lb+iSeeMKKjo42PP/7YWLVqlXHxxRcbHTp0MEpLS11lRowYYfTp08f4+eefje+//97o3LmzcfXVV7vW5+fnG4mJicbo0aONtWvXGu+8844RFhZm/Pe//3WV+fHHHw2bzWY89dRTxvr164377rvPCA4ONtasWeP1fwM0L8OHDzdee+01Y+3atcbKlSuNCy+80GjXrp1RVFTkKnPTTTcZKSkpxty5c41ly5YZp59+unHGGWe41ldVVRk9e/Y0MjIyjF9++cX48ssvjbi4OGPy5MmuMlu3bjXCw8ONSZMmGevXrzdeeOEFw2azGbNnz3aV4XcC3vbpp58aX3zxhfHrr78amZmZxt/+9jcjODjYWLt2rWEYnOsIbEuWLDFSU1ON3r17G7fddptrOec9mgJJuo8aNGiQMWHCBNd7h8NhJCcnG1OnTjUxKqBuxybpTqfTSEpKMp5++mnXsry8PMNutxvvvPOOYRiGsX79ekOSsXTpUleZr776yrBYLMaePXsMwzCM//znP0ZsbKxRXl7uKnPPPfcYXbt2db2/8sorjZEjR7rFk5aWZvz5z3/26DECx8rJyTEkGQsXLjQMo/ocDw4ONt5//31XmQ0bNhiSjEWLFhmGUf1wy2q1GllZWa4yL774ohEVFeU6z++++27j1FNPdfusq666yhg+fLjrPb8TMENsbKzxyiuvcK4joBUWFhpdunQx5syZYwwZMsSVpHPeo6nQ3N0HVVRUaPny5crIyHAts1qtysjI0KJFi0yMDGi4bdu2KSsry+08jo6OVlpamus8XrRokWJiYjRgwABXmYyMDFmtVi1evNhV5uyzz1ZISIirzPDhw5WZmalDhw65yhz9OUfK8H2Bt+Xn50uSWrZsKUlavny5Kisr3c7Hbt26qV27dm7nfa9evZSYmOgqM3z4cBUUFGjdunWuMvWd0/xOoKk5HA7NmjVLxcXFSk9P51xHQJswYYJGjhxZ49zkvEdTCTI7ANR04MABORwOty+3JCUmJmrjxo0mRQU0TlZWliTVeh4fWZeVlaWEhAS39UFBQWrZsqVbmQ4dOtTYx5F1sbGxysrKqvdzAG9wOp26/fbbdeaZZ6pnz56Sqs/JkJAQxcTEuJU99ryv7Xw9sq6+MgUFBSotLdWhQ4f4nUCTWLNmjdLT01VWVqaIiAh99NFH6tGjh1auXMm5joA0a9YsrVixQkuXLq2xjms8mgpJOgAAJ2DChAlau3atfvjhB7NDAbyma9euWrlypfLz8/XBBx/ouuuu08KFC80OC/CKXbt26bbbbtOcOXMUGhpqdjhoxmju7oPi4uJks9lqjBSZnZ2tpKQkk6ICGufIuVrfeZyUlKScnBy39VVVVcrNzXUrU9s+jv6MusrwfYG3TJw4UZ9//rnmz5+vtm3bupYnJSWpoqJCeXl5buWPPe9P9JyOiopSWFgYvxNoMiEhIercubP69++vqVOnqk+fPvrXv/7FuY6AtHz5cuXk5Oi0005TUFCQgoKCtHDhQj3//PMKCgpSYmIi5z2aBEm6DwoJCVH//v01d+5c1zKn06m5c+cqPT3dxMiAhuvQoYOSkpLczuOCggItXrzYdR6np6crLy9Py5cvd5WZN2+enE6n0tLSXGW+++47VVZWusrMmTNHXbt2VWxsrKvM0Z9zpAzfF3iaYRiaOHGiPvroI82bN69GV4z+/fsrODjY7XzMzMzUzp073c77NWvWuD2gmjNnjqKiotSjRw9XmfrOaX4nYBan06ny8nLOdQSkoUOHas2aNVq5cqXrNWDAAI0ePdr1N+c9moTZI9ehdrNmzTLsdrvx+uuvG+vXrzfGjx9vxMTEuI0UCZitsLDQ+OWXX4xffvnFkGQ8++yzxi+//GLs2LHDMIzqKdhiYmKMTz75xFi9erVxySWX1DoFW79+/YzFixcbP/zwg9GlSxe3Kdjy8vKMxMRE49prrzXWrl1rzJo1ywgPD68xBVtQUJDxzDPPGBs2bDAefPBBpmCDV9x8881GdHS0sWDBAmPfvn2uV0lJiavMTTfdZLRr186YN2+esWzZMiM9Pd1IT093rT8yPc+wYcOMlStXGrNnzzbi4+NrnZ7nrrvuMjZs2GBMmzat1ul5+J2AN917773GwoULjW3bthmrV6827r33XsNisRjffPONYRic62gejh7d3TA479E0SNJ92AsvvGC0a9fOCAkJMQYNGmT8/PPPZocEuJk/f74hqcbruuuuMwyjehq2+++/30hMTDTsdrsxdOhQIzMz020fBw8eNK6++mojIiLCiIqKMsaOHWsUFha6lVm1apUxePBgw263G23atDGeeOKJGrG89957ximnnGKEhIQYp556qvHFF1947bjRfNV2vksyXnvtNVeZ0tJS45ZbbjFiY2ON8PBw47LLLjP27dvntp/t27cbF1xwgREWFmbExcUZd9xxh1FZWelWZv78+Ubfvn2NkJAQo2PHjm6fcQS/E/CmP/3pT0b79u2NkJAQIz4+3hg6dKgrQTcMznU0D8cm6Zz3aAoWwzAMc+rwAQAAAADA0eiTDgAAAACAjyBJBwAAAADAR5CkAwAAAADgI0jSAQAAAADwESTpAAAAAAD4CJJ0AAAAAAB8BEk6AAAAAAA+giQdAAAAAAAfQZIOAAAAAICPIEkHAAAAAMBHkKQDAAAAAOAj/h8reZD1XcXV3QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**<br/>\n",
        "Here Librosa converts the signal to mono, meaning the channel will always be one. (stereo means 2 channels)"
      ],
      "metadata": {
        "id": "qXZaZkhPvPfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile as wav\n",
        "wave_sample_rate, wave_audio= wav.read(audio_file_path)"
      ],
      "metadata": {
        "id": "Vv_fvAYOvEPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wave_audio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB48BQhIv0xz",
        "outputId": "ac3a77c9-782e-463d-fed9-4c88015b34ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 256,  256],\n",
              "       [ 256,  256],\n",
              "       [   0,    0],\n",
              "       ...,\n",
              "       [ 256, 1280],\n",
              "       [ 512, 1024],\n",
              "       [ 256,  512]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(wave_audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "DI73GuHtv2k-",
        "outputId": "54566432-b7e4-4eda-e304-4d03fe40dd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79d32b83a2f0>,\n",
              " <matplotlib.lines.Line2D at 0x79d32b83a350>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Features**<br/>\n",
        "Here we will be using Mel-Frequency Cepstral Coefficients (MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyze both the frequency and the time charateristics of the sound. The audio representations will allow us to identify features for classification."
      ],
      "metadata": {
        "id": "1JYJFDHBwLh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs= librosa.feature.mfcc(y=librosa_audio_data, sr= librosa_sample_rate, n_mfcc=40)\n",
        "print(mfccs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kosGorLXwC3d",
        "outputId": "414a367e-6a66-44ba-dd42-68c467d1119b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 88)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gcvO4Oray119",
        "outputId": "45d560a7-43bd-4961-9e79-17e50d41a45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      File_Name  Start_Time  End_Time  Class\n",
              "0  angry_22.wav           0  2.080000  angry\n",
              "1  happy_45.wav           0  5.000000  happy\n",
              "2   angry_8.wav           0  8.262676  angry\n",
              "3   angry_9.wav           0  4.000000  angry\n",
              "4  happy_47.wav           0  1.890000  happy"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4ddf4a3-8d11-4c4f-a6cf-16a74dc5d781\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Start_Time</th>\n",
              "      <th>End_Time</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>angry_22.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>2.080000</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>happy_45.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>angry_8.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>8.262676</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>angry_9.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>happy_47.wav</td>\n",
              "      <td>0</td>\n",
              "      <td>1.890000</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4ddf4a3-8d11-4c4f-a6cf-16a74dc5d781')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4ddf4a3-8d11-4c4f-a6cf-16a74dc5d781 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4ddf4a3-8d11-4c4f-a6cf-16a74dc5d781');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f55911dd-4c0e-4eee-92b4-655b979dfd65\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f55911dd-4c0e-4eee-92b4-655b979dfd65')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f55911dd-4c0e-4eee-92b4-655b979dfd65 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metadata_df",
              "summary": "{\n  \"name\": \"metadata_df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"File_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"howling_17.wav\",\n          \"sad_33.wav\",\n          \"angry_19.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Start_Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"End_Time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.618098132109122,\n        \"min\": 1.0,\n        \"max\": 58.896,\n        \"num_unique_values\": 142,\n        \"samples\": [\n          8.39997732426304\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"happy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extractor(file):\n",
        "  audio, sample_rate= librosa.load(file, res_type='kaiser_fast')\n",
        "  mfccs_features= librosa.feature.mfcc(y=audio, sr= sample_rate, n_mfcc=40)\n",
        "  mfccs_scaled_features= np.mean(mfccs_features.T , axis=0)\n",
        "  return mfccs_scaled_features"
      ],
      "metadata": {
        "id": "2k2DMcuE5B7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade resampy"
      ],
      "metadata": {
        "id": "ERtSnh0KCVph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6389c30-29a6-44b3-ef45-89947ce619c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting resampy\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.25.2)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.41.1)\n",
            "Installing collected packages: resampy\n",
            "Successfully installed resampy-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import resampy\n",
        "\n",
        "extracted_features= []\n",
        "for index_num, row in tqdm(metadata_df.iterrows()):\n",
        "  file_name= os.path.join(os.path.abspath(path)+'/',str(row[\"File_Name\"]))\n",
        "  final_class_labels= row[\"Class\"]\n",
        "  data= features_extractor(file_name)\n",
        "  extracted_features.append([data, final_class_labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJAI3_at6lVi",
        "outputId": "b52cc990-dbdd-4b8f-fcd4-12b901042e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:50,  3.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show resampy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50vNxaJjwTv7",
        "outputId": "8e9109f6-9603-47af-8edf-8aeb5b5d2614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: resampy\n",
            "Version: 0.4.2\n",
            "Summary: Efficient signal resampling\n",
            "Home-page: https://github.com/bmcfee/resampy\n",
            "Author: Brian McFee\n",
            "Author-email: brian.mcfee@nyu.edu\n",
            "License: ISC\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numba, numpy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_features_df= pd.DataFrame(extracted_features, columns=['feature', 'class'])\n",
        "extracted_features_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "J5QjtKdkBjRF",
        "outputId": "6b676ca4-9838-41d8-aca3-7c705dd3c835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature  class\n",
              "0  [-303.87823, 133.94948, -20.448854, 24.511597,...  angry\n",
              "1  [-569.43396, 31.936527, 0.65314204, 1.6708603,...  happy\n",
              "2  [-396.4329, 76.25201, 27.272411, 8.170999, 2.4...  angry\n",
              "3  [-403.45078, 93.77245, 17.812965, -11.735519, ...  angry\n",
              "4  [-274.98367, 112.529, -34.67252, -19.668827, -...  happy"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad9b339a-b3b1-499f-9854-d633ed4fe4ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-303.87823, 133.94948, -20.448854, 24.511597,...</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-569.43396, 31.936527, 0.65314204, 1.6708603,...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-396.4329, 76.25201, 27.272411, 8.170999, 2.4...</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-403.45078, 93.77245, 17.812965, -11.735519, ...</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-274.98367, 112.529, -34.67252, -19.668827, -...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad9b339a-b3b1-499f-9854-d633ed4fe4ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad9b339a-b3b1-499f-9854-d633ed4fe4ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad9b339a-b3b1-499f-9854-d633ed4fe4ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d40efe07-30c2-4770-83c7-296a30c8ef12\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d40efe07-30c2-4770-83c7-296a30c8ef12')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d40efe07-30c2-4770-83c7-296a30c8ef12 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "extracted_features_df",
              "summary": "{\n  \"name\": \"extracted_features_df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"feature\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"happy\",\n          \"howling\",\n          \"angry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split tyhe dataset into idependent and dependent dataset\n",
        "X= np.array(extracted_features_df['feature'].tolist())\n",
        "y= np.array(extracted_features_df['class'].tolist())"
      ],
      "metadata": {
        "id": "wanEv8R8E5Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdUfJfIWFhwN",
        "outputId": "8f20d432-3237-43ff-978f-946eaaed8c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= np.array(pd.get_dummies(y))"
      ],
      "metadata": {
        "id": "1XXUakGLFo4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jscKWrkaGapV",
        "outputId": "4f6f7a99-b109-4177-adce-3d21428b8e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "Rx9jnPo4F3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Mmd8xUHgd-",
        "outputId": "fbe97ff4-3d77-4ab0-9b66-dea49754f9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.3115625e+02,  8.1865494e+01, -1.2817248e+01, ...,\n",
              "        -1.6098765e+00, -1.4386885e+00, -1.5625091e+00],\n",
              "       [-2.9960861e+02,  4.1762726e+01, -7.8915611e+01, ...,\n",
              "         6.1729956e-01,  1.4543340e+00,  1.5793740e+00],\n",
              "       [-2.6963950e+02,  1.5330043e+02, -1.2997303e+01, ...,\n",
              "        -2.6407614e+00, -3.2527335e+00, -3.6500838e+00],\n",
              "       ...,\n",
              "       [-3.7968338e+02,  7.4177124e+01, -2.1628279e+01, ...,\n",
              "         2.1048361e-01, -6.4544529e-01, -4.5857009e-01],\n",
              "       [-6.5077393e+01,  1.2843613e+02, -7.5756088e+01, ...,\n",
              "        -8.1372142e-01,  3.5030572e+00,  2.5042528e-01],\n",
              "       [-3.6988953e+02,  1.7578954e+02, -1.6954502e+01, ...,\n",
              "        -3.0032570e+00, -1.0623419e+00, -2.2376134e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTw4-f5ZHivR",
        "outputId": "ae016b9b-6afe-447f-f6ab-29069462bfb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 0, 1],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [0, 1, 0, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGCxGUpLG9zU",
        "outputId": "60a8f1a4-2334-4391-9bba-1fe1289a95bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX9psQauG_m6",
        "outputId": "0ebceb31-4ae4-4ee5-cef5-6ebc28e9470b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVpbsh_sHBQs",
        "outputId": "a1e018f9-888b-46d6-e6c1-ba2f26ed9b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToI9Qzk9HE5o",
        "outputId": "045aa4a2-8151-4e46-eea0-b279e22b265c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation"
      ],
      "metadata": {
        "id": "_lQbGH3lHwhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T2K73KvHHFg",
        "outputId": "a4f4826b-fceb-4a4f-da41-b2831c0be2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "kFUbCAWtH6mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels= y.shape[1]"
      ],
      "metadata": {
        "id": "afJXRMBBIodj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "#first layer\n",
        "model.add(Dense(100, input_shape=(40,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#second layer\n",
        "model.add(Dense(200))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#third layer\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#final layer\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "morn5Ck4IvZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn9sx-trKa6L",
        "outputId": "01eff381-f351-435b-f6c0-ea80b003add4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               4100      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 100)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               20200     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 200)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 404       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44804 (175.02 KB)\n",
            "Trainable params: 44804 (175.02 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= 'adam')"
      ],
      "metadata": {
        "id": "grMB4qGqKd-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training my model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "\n",
        "num_epochs = 400\n",
        "num_batch_size= 30\n",
        "\n",
        "checkpointer= ModelCheckpoint(filepath='saved_model/dog_audio_emotion.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "start= datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size= num_batch_size, epochs= num_epochs, validation_data=(X_test, y_test), callbacks= [checkpointer])\n",
        "\n",
        "duration= datetime.now() - start\n",
        "print(\"Training completed in time \",duration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9tIvfBK-eC",
        "outputId": "057698f7-d19b-49da-dc08-c7f8ca27f472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 1: val_loss improved from inf to 2.34825, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 58ms/step - loss: 0.0426 - accuracy: 0.9875 - val_loss: 2.3482 - val_accuracy: 0.8500\n",
            "Epoch 2/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0233 - accuracy: 1.0000\n",
            "Epoch 2: val_loss improved from 2.34825 to 2.20075, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0470 - accuracy: 0.9875 - val_loss: 2.2007 - val_accuracy: 0.8750\n",
            "Epoch 3/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 3: val_loss improved from 2.20075 to 2.17345, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 53ms/step - loss: 0.0608 - accuracy: 0.9875 - val_loss: 2.1734 - val_accuracy: 0.8750\n",
            "Epoch 4/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 4: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.0518 - accuracy: 0.9875 - val_loss: 2.2676 - val_accuracy: 0.8750\n",
            "Epoch 5/400\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9937\n",
            "Epoch 5: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0271 - accuracy: 0.9937 - val_loss: 2.3955 - val_accuracy: 0.8250\n",
            "Epoch 6/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 6: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 2.4413 - val_accuracy: 0.8250\n",
            "Epoch 7/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 2.4499 - val_accuracy: 0.8250\n",
            "Epoch 8/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1302 - accuracy: 0.9333\n",
            "Epoch 8: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0543 - accuracy: 0.9812 - val_loss: 2.4901 - val_accuracy: 0.8250\n",
            "Epoch 9/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 9: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 46ms/step - loss: 0.0784 - accuracy: 0.9812 - val_loss: 2.5410 - val_accuracy: 0.8250\n",
            "Epoch 10/400\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.0786 - accuracy: 0.9667\n",
            "Epoch 10: val_loss did not improve from 2.17345\n",
            "6/6 [==============================] - 0s 47ms/step - loss: 0.0737 - accuracy: 0.9688 - val_loss: 2.3459 - val_accuracy: 0.8500\n",
            "Epoch 11/400\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9812\n",
            "Epoch 11: val_loss improved from 2.17345 to 2.13374, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 0.0875 - accuracy: 0.9812 - val_loss: 2.1337 - val_accuracy: 0.8500\n",
            "Epoch 12/400\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9750\n",
            "Epoch 12: val_loss improved from 2.13374 to 2.05305, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.0844 - accuracy: 0.9750 - val_loss: 2.0530 - val_accuracy: 0.8500\n",
            "Epoch 13/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 13: val_loss did not improve from 2.05305\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0655 - accuracy: 0.9812 - val_loss: 2.0557 - val_accuracy: 0.8250\n",
            "Epoch 14/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0848 - accuracy: 0.9667\n",
            "Epoch 14: val_loss did not improve from 2.05305\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0549 - accuracy: 0.9875 - val_loss: 2.0682 - val_accuracy: 0.8750\n",
            "Epoch 15/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0485 - accuracy: 0.9667\n",
            "Epoch 15: val_loss did not improve from 2.05305\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0208 - accuracy: 0.9937 - val_loss: 2.1202 - val_accuracy: 0.8750\n",
            "Epoch 16/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0372 - accuracy: 0.9667\n",
            "Epoch 16: val_loss did not improve from 2.05305\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0681 - accuracy: 0.9812 - val_loss: 2.1333 - val_accuracy: 0.8500\n",
            "Epoch 17/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0258 - accuracy: 1.0000\n",
            "Epoch 17: val_loss did not improve from 2.05305\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0908 - accuracy: 0.9750 - val_loss: 2.0636 - val_accuracy: 0.8750\n",
            "Epoch 18/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0786 - accuracy: 0.9667\n",
            "Epoch 18: val_loss improved from 2.05305 to 2.05267, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0544 - accuracy: 0.9812 - val_loss: 2.0527 - val_accuracy: 0.8750\n",
            "Epoch 19/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 19: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1077 - accuracy: 0.9688 - val_loss: 2.1019 - val_accuracy: 0.8500\n",
            "Epoch 20/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9667\n",
            "Epoch 20: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0372 - accuracy: 0.9937 - val_loss: 2.1354 - val_accuracy: 0.8750\n",
            "Epoch 21/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 21: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0685 - accuracy: 0.9812 - val_loss: 2.1648 - val_accuracy: 0.8500\n",
            "Epoch 22/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 22: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0838 - accuracy: 0.9750 - val_loss: 2.1989 - val_accuracy: 0.8500\n",
            "Epoch 23/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 23: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9875 - val_loss: 2.2312 - val_accuracy: 0.8500\n",
            "Epoch 24/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9667\n",
            "Epoch 24: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0944 - accuracy: 0.9688 - val_loss: 2.3318 - val_accuracy: 0.8500\n",
            "Epoch 25/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0673 - accuracy: 0.9667\n",
            "Epoch 25: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0621 - accuracy: 0.9750 - val_loss: 2.2875 - val_accuracy: 0.8250\n",
            "Epoch 26/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0909 - accuracy: 0.9667\n",
            "Epoch 26: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0338 - accuracy: 0.9937 - val_loss: 2.3149 - val_accuracy: 0.8500\n",
            "Epoch 27/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1993 - accuracy: 0.9000\n",
            "Epoch 27: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0911 - accuracy: 0.9625 - val_loss: 2.3985 - val_accuracy: 0.8500\n",
            "Epoch 28/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1035 - accuracy: 0.9667\n",
            "Epoch 28: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0563 - accuracy: 0.9875 - val_loss: 2.3659 - val_accuracy: 0.8500\n",
            "Epoch 29/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1082 - accuracy: 0.9667\n",
            "Epoch 29: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0764 - accuracy: 0.9625 - val_loss: 2.3496 - val_accuracy: 0.8750\n",
            "Epoch 30/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0727 - accuracy: 0.9688 - val_loss: 2.4544 - val_accuracy: 0.8500\n",
            "Epoch 31/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0863 - accuracy: 0.9667\n",
            "Epoch 31: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0737 - accuracy: 0.9625 - val_loss: 2.5464 - val_accuracy: 0.8500\n",
            "Epoch 32/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.3387 - accuracy: 0.9333\n",
            "Epoch 32: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0787 - accuracy: 0.9812 - val_loss: 2.6815 - val_accuracy: 0.8250\n",
            "Epoch 33/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1877 - accuracy: 0.9333\n",
            "Epoch 33: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0930 - accuracy: 0.9688 - val_loss: 2.7276 - val_accuracy: 0.8500\n",
            "Epoch 34/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0565 - accuracy: 0.9667\n",
            "Epoch 34: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0935 - accuracy: 0.9688 - val_loss: 2.6356 - val_accuracy: 0.8500\n",
            "Epoch 35/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1001 - accuracy: 0.9667\n",
            "Epoch 35: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0613 - accuracy: 0.9750 - val_loss: 2.6220 - val_accuracy: 0.8500\n",
            "Epoch 36/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0976 - accuracy: 0.9333\n",
            "Epoch 36: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0623 - accuracy: 0.9750 - val_loss: 2.6613 - val_accuracy: 0.8250\n",
            "Epoch 37/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1628 - accuracy: 0.9333\n",
            "Epoch 37: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1023 - accuracy: 0.9563 - val_loss: 2.4540 - val_accuracy: 0.8750\n",
            "Epoch 38/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0871 - accuracy: 0.9667\n",
            "Epoch 38: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9875 - val_loss: 2.3725 - val_accuracy: 0.8750\n",
            "Epoch 39/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.3433 - accuracy: 0.8667\n",
            "Epoch 39: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1406 - accuracy: 0.9563 - val_loss: 2.1477 - val_accuracy: 0.9000\n",
            "Epoch 40/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1751 - accuracy: 0.9333\n",
            "Epoch 40: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2151 - accuracy: 0.9250 - val_loss: 2.2628 - val_accuracy: 0.9000\n",
            "Epoch 41/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0824 - accuracy: 0.9667\n",
            "Epoch 41: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0802 - accuracy: 0.9688 - val_loss: 2.4357 - val_accuracy: 0.8750\n",
            "Epoch 42/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0180 - accuracy: 1.0000\n",
            "Epoch 42: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0403 - accuracy: 0.9937 - val_loss: 2.6149 - val_accuracy: 0.8500\n",
            "Epoch 43/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2598 - accuracy: 0.8667\n",
            "Epoch 43: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1367 - accuracy: 0.9438 - val_loss: 2.5795 - val_accuracy: 0.8500\n",
            "Epoch 44/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0544 - accuracy: 0.9667\n",
            "Epoch 44: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1278 - accuracy: 0.9563 - val_loss: 2.2463 - val_accuracy: 0.9000\n",
            "Epoch 45/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 45: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0682 - accuracy: 0.9812 - val_loss: 2.2060 - val_accuracy: 0.9000\n",
            "Epoch 46/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 46: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0832 - accuracy: 0.9625 - val_loss: 2.1813 - val_accuracy: 0.9000\n",
            "Epoch 47/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0981 - accuracy: 0.9333\n",
            "Epoch 47: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0782 - accuracy: 0.9625 - val_loss: 2.1553 - val_accuracy: 0.9000\n",
            "Epoch 48/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
            "Epoch 48: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0426 - accuracy: 0.9875 - val_loss: 2.1474 - val_accuracy: 0.9000\n",
            "Epoch 49/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1218 - accuracy: 0.9333\n",
            "Epoch 49: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0761 - accuracy: 0.9688 - val_loss: 2.1437 - val_accuracy: 0.9000\n",
            "Epoch 50/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1005 - accuracy: 0.9667\n",
            "Epoch 50: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 2.1409 - val_accuracy: 0.9000\n",
            "Epoch 51/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0788 - accuracy: 0.9667\n",
            "Epoch 51: val_loss did not improve from 2.05267\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1394 - accuracy: 0.9688 - val_loss: 2.1013 - val_accuracy: 0.9000\n",
            "Epoch 52/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0764 - accuracy: 0.9667\n",
            "Epoch 52: val_loss improved from 2.05267 to 2.02741, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0447 - accuracy: 0.9875 - val_loss: 2.0274 - val_accuracy: 0.8750\n",
            "Epoch 53/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0778 - accuracy: 1.0000\n",
            "Epoch 53: val_loss did not improve from 2.02741\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0547 - accuracy: 0.9937 - val_loss: 2.0464 - val_accuracy: 0.8750\n",
            "Epoch 54/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 54: val_loss did not improve from 2.02741\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0540 - accuracy: 0.9812 - val_loss: 2.0841 - val_accuracy: 0.8750\n",
            "Epoch 55/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1994 - accuracy: 0.9667\n",
            "Epoch 55: val_loss did not improve from 2.02741\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0944 - accuracy: 0.9875 - val_loss: 2.1249 - val_accuracy: 0.8750\n",
            "Epoch 56/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1448 - accuracy: 0.9333\n",
            "Epoch 56: val_loss did not improve from 2.02741\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0980 - accuracy: 0.9563 - val_loss: 2.1359 - val_accuracy: 0.8750\n",
            "Epoch 57/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1181 - accuracy: 0.9333\n",
            "Epoch 57: val_loss improved from 2.02741 to 2.02664, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0736 - accuracy: 0.9688 - val_loss: 2.0266 - val_accuracy: 0.8750\n",
            "Epoch 58/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 58: val_loss improved from 2.02664 to 1.99957, saving model to saved_model/dog_audio_emotion.hdf5\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1005 - accuracy: 0.9812 - val_loss: 1.9996 - val_accuracy: 0.8750\n",
            "Epoch 59/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1289 - accuracy: 0.9333\n",
            "Epoch 59: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0767 - accuracy: 0.9688 - val_loss: 2.0448 - val_accuracy: 0.8750\n",
            "Epoch 60/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0781 - accuracy: 0.9667\n",
            "Epoch 60: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0772 - accuracy: 0.9688 - val_loss: 2.1145 - val_accuracy: 0.8750\n",
            "Epoch 61/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0681 - accuracy: 0.9667\n",
            "Epoch 61: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0492 - accuracy: 0.9875 - val_loss: 2.2013 - val_accuracy: 0.8750\n",
            "Epoch 62/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 62: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0557 - accuracy: 0.9812 - val_loss: 2.2827 - val_accuracy: 0.8500\n",
            "Epoch 63/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0897 - accuracy: 0.9333\n",
            "Epoch 63: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0466 - accuracy: 0.9750 - val_loss: 2.3192 - val_accuracy: 0.8500\n",
            "Epoch 64/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 64: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1125 - accuracy: 0.9563 - val_loss: 2.3019 - val_accuracy: 0.8500\n",
            "Epoch 65/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 65: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0515 - accuracy: 0.9875 - val_loss: 2.3370 - val_accuracy: 0.8500\n",
            "Epoch 66/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 66: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0762 - accuracy: 0.9812 - val_loss: 2.3816 - val_accuracy: 0.8500\n",
            "Epoch 67/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2146 - accuracy: 0.9333\n",
            "Epoch 67: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1664 - accuracy: 0.9625 - val_loss: 2.4873 - val_accuracy: 0.8000\n",
            "Epoch 68/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 68: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0549 - accuracy: 0.9688 - val_loss: 2.5775 - val_accuracy: 0.8000\n",
            "Epoch 69/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 69: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0530 - accuracy: 0.9812 - val_loss: 2.6032 - val_accuracy: 0.8000\n",
            "Epoch 70/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0902 - accuracy: 0.9333\n",
            "Epoch 70: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0831 - accuracy: 0.9563 - val_loss: 2.4266 - val_accuracy: 0.8250\n",
            "Epoch 71/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 71: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 0.9937 - val_loss: 2.3602 - val_accuracy: 0.8750\n",
            "Epoch 72/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 72: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 2.3311 - val_accuracy: 0.9000\n",
            "Epoch 73/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 73: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0614 - accuracy: 0.9812 - val_loss: 2.2952 - val_accuracy: 0.9000\n",
            "Epoch 74/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0751 - accuracy: 0.9333\n",
            "Epoch 74: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0795 - accuracy: 0.9812 - val_loss: 2.2410 - val_accuracy: 0.9000\n",
            "Epoch 75/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0297 - accuracy: 0.9667\n",
            "Epoch 75: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0788 - accuracy: 0.9812 - val_loss: 2.2027 - val_accuracy: 0.9000\n",
            "Epoch 76/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2014 - accuracy: 0.9667\n",
            "Epoch 76: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0788 - accuracy: 0.9750 - val_loss: 2.2151 - val_accuracy: 0.9000\n",
            "Epoch 77/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 77: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0353 - accuracy: 0.9875 - val_loss: 2.2782 - val_accuracy: 0.8750\n",
            "Epoch 78/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0609 - accuracy: 1.0000\n",
            "Epoch 78: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0547 - accuracy: 0.9750 - val_loss: 2.3188 - val_accuracy: 0.8750\n",
            "Epoch 79/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 79: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1035 - accuracy: 0.9875 - val_loss: 2.3966 - val_accuracy: 0.8750\n",
            "Epoch 80/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 80: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0597 - accuracy: 0.9812 - val_loss: 2.4675 - val_accuracy: 0.8500\n",
            "Epoch 81/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 81: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0906 - accuracy: 0.9688 - val_loss: 2.4887 - val_accuracy: 0.8500\n",
            "Epoch 82/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 82: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0832 - accuracy: 0.9625 - val_loss: 2.4557 - val_accuracy: 0.8500\n",
            "Epoch 83/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 83: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0511 - accuracy: 0.9812 - val_loss: 2.4872 - val_accuracy: 0.8750\n",
            "Epoch 84/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 84: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0525 - accuracy: 0.9750 - val_loss: 2.5870 - val_accuracy: 0.8500\n",
            "Epoch 85/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9667\n",
            "Epoch 85: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0395 - accuracy: 0.9812 - val_loss: 2.6305 - val_accuracy: 0.8500\n",
            "Epoch 86/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 86: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0582 - accuracy: 0.9875 - val_loss: 2.6901 - val_accuracy: 0.8500\n",
            "Epoch 87/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1088 - accuracy: 0.9667\n",
            "Epoch 87: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0615 - accuracy: 0.9812 - val_loss: 2.6512 - val_accuracy: 0.8750\n",
            "Epoch 88/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 88: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0307 - accuracy: 0.9875 - val_loss: 2.6157 - val_accuracy: 0.8750\n",
            "Epoch 89/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0759 - accuracy: 0.9667\n",
            "Epoch 89: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0702 - accuracy: 0.9688 - val_loss: 2.6024 - val_accuracy: 0.8750\n",
            "Epoch 90/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 90: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0468 - accuracy: 0.9875 - val_loss: 2.5756 - val_accuracy: 0.8750\n",
            "Epoch 91/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 91: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0464 - accuracy: 0.9750 - val_loss: 2.5958 - val_accuracy: 0.9000\n",
            "Epoch 92/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 92: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0371 - accuracy: 0.9875 - val_loss: 2.6196 - val_accuracy: 0.8750\n",
            "Epoch 93/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 9.0937e-04 - accuracy: 1.0000\n",
            "Epoch 93: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0331 - accuracy: 0.9812 - val_loss: 2.6059 - val_accuracy: 0.8750\n",
            "Epoch 94/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0888 - accuracy: 0.9667\n",
            "Epoch 94: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0350 - accuracy: 0.9937 - val_loss: 2.6099 - val_accuracy: 0.8750\n",
            "Epoch 95/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 95: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0179 - accuracy: 0.9937 - val_loss: 2.6238 - val_accuracy: 0.8750\n",
            "Epoch 96/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 96: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0488 - accuracy: 0.9875 - val_loss: 2.6275 - val_accuracy: 0.8750\n",
            "Epoch 97/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 97: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0474 - accuracy: 0.9875 - val_loss: 2.5255 - val_accuracy: 0.9000\n",
            "Epoch 98/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
            "Epoch 98: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0602 - accuracy: 0.9750 - val_loss: 2.4958 - val_accuracy: 0.9000\n",
            "Epoch 99/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 99: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0251 - accuracy: 0.9875 - val_loss: 2.4882 - val_accuracy: 0.9000\n",
            "Epoch 100/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0450 - accuracy: 0.9667\n",
            "Epoch 100: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0620 - accuracy: 0.9750 - val_loss: 2.4923 - val_accuracy: 0.9000\n",
            "Epoch 101/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 101: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.0356 - accuracy: 0.9937 - val_loss: 2.5028 - val_accuracy: 0.9000\n",
            "Epoch 102/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9667\n",
            "Epoch 102: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0254 - accuracy: 0.9875 - val_loss: 2.5419 - val_accuracy: 0.9000\n",
            "Epoch 103/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1223 - accuracy: 0.9667\n",
            "Epoch 103: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0355 - accuracy: 0.9937 - val_loss: 2.5647 - val_accuracy: 0.9000\n",
            "Epoch 104/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 104: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0302 - accuracy: 0.9875 - val_loss: 2.6132 - val_accuracy: 0.9000\n",
            "Epoch 105/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0523 - accuracy: 1.0000\n",
            "Epoch 105: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0293 - accuracy: 0.9937 - val_loss: 2.6772 - val_accuracy: 0.9000\n",
            "Epoch 106/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 106: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0305 - accuracy: 0.9937 - val_loss: 2.7501 - val_accuracy: 0.9000\n",
            "Epoch 107/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 107: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0463 - accuracy: 0.9875 - val_loss: 2.8339 - val_accuracy: 0.8750\n",
            "Epoch 108/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0470 - accuracy: 0.9667\n",
            "Epoch 108: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0654 - accuracy: 0.9625 - val_loss: 2.7828 - val_accuracy: 0.8750\n",
            "Epoch 109/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 109: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0250 - accuracy: 0.9937 - val_loss: 2.6247 - val_accuracy: 0.8750\n",
            "Epoch 110/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 110: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1034 - accuracy: 0.9750 - val_loss: 2.5851 - val_accuracy: 0.8750\n",
            "Epoch 111/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9333\n",
            "Epoch 111: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0357 - accuracy: 0.9875 - val_loss: 2.6193 - val_accuracy: 0.8750\n",
            "Epoch 112/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 112: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0436 - accuracy: 0.9875 - val_loss: 2.6714 - val_accuracy: 0.8500\n",
            "Epoch 113/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0961 - accuracy: 0.9667\n",
            "Epoch 113: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0602 - accuracy: 0.9750 - val_loss: 2.7055 - val_accuracy: 0.8500\n",
            "Epoch 114/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0666 - accuracy: 0.9333\n",
            "Epoch 114: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0569 - accuracy: 0.9688 - val_loss: 2.7099 - val_accuracy: 0.8500\n",
            "Epoch 115/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1909 - accuracy: 0.9667\n",
            "Epoch 115: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0884 - accuracy: 0.9875 - val_loss: 2.6761 - val_accuracy: 0.8500\n",
            "Epoch 116/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9667\n",
            "Epoch 116: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0510 - accuracy: 0.9750 - val_loss: 2.6713 - val_accuracy: 0.8500\n",
            "Epoch 117/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1000 - accuracy: 0.9667\n",
            "Epoch 117: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0435 - accuracy: 0.9875 - val_loss: 2.6892 - val_accuracy: 0.8500\n",
            "Epoch 118/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 118: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0432 - accuracy: 0.9875 - val_loss: 2.6874 - val_accuracy: 0.8750\n",
            "Epoch 119/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 119: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0305 - accuracy: 0.9937 - val_loss: 2.6953 - val_accuracy: 0.8750\n",
            "Epoch 120/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 120: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0479 - accuracy: 0.9812 - val_loss: 2.7192 - val_accuracy: 0.8750\n",
            "Epoch 121/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1690 - accuracy: 0.9333\n",
            "Epoch 121: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0763 - accuracy: 0.9563 - val_loss: 2.7416 - val_accuracy: 0.8750\n",
            "Epoch 122/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 122: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0271 - accuracy: 0.9937 - val_loss: 2.8124 - val_accuracy: 0.8750\n",
            "Epoch 123/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0772 - accuracy: 0.9667\n",
            "Epoch 123: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0929 - accuracy: 0.9688 - val_loss: 2.7929 - val_accuracy: 0.8750\n",
            "Epoch 124/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 124: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0452 - accuracy: 0.9875 - val_loss: 2.7875 - val_accuracy: 0.8750\n",
            "Epoch 125/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1573 - accuracy: 0.9333\n",
            "Epoch 125: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0713 - accuracy: 0.9750 - val_loss: 2.7824 - val_accuracy: 0.9000\n",
            "Epoch 126/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 126: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0496 - accuracy: 0.9750 - val_loss: 2.7880 - val_accuracy: 0.9000\n",
            "Epoch 127/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 127: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0578 - accuracy: 0.9688 - val_loss: 2.7910 - val_accuracy: 0.8750\n",
            "Epoch 128/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 128: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0536 - accuracy: 0.9812 - val_loss: 2.8186 - val_accuracy: 0.8750\n",
            "Epoch 129/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.4479 - accuracy: 0.9000\n",
            "Epoch 129: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1030 - accuracy: 0.9750 - val_loss: 2.8845 - val_accuracy: 0.8750\n",
            "Epoch 130/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
            "Epoch 130: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0995 - accuracy: 0.9812 - val_loss: 2.9810 - val_accuracy: 0.8500\n",
            "Epoch 131/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0874 - accuracy: 0.9333\n",
            "Epoch 131: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9750 - val_loss: 3.0025 - val_accuracy: 0.8500\n",
            "Epoch 132/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 8.3897e-04 - accuracy: 1.0000\n",
            "Epoch 132: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 2.9412 - val_accuracy: 0.8500\n",
            "Epoch 133/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 133: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 2.9479 - val_accuracy: 0.8500\n",
            "Epoch 134/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 134: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 0.9812 - val_loss: 2.9788 - val_accuracy: 0.8500\n",
            "Epoch 135/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.5434 - accuracy: 0.9667\n",
            "Epoch 135: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1286 - accuracy: 0.9812 - val_loss: 2.8274 - val_accuracy: 0.8250\n",
            "Epoch 136/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0193 - accuracy: 1.0000\n",
            "Epoch 136: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0525 - accuracy: 0.9812 - val_loss: 2.7738 - val_accuracy: 0.8250\n",
            "Epoch 137/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0200 - accuracy: 1.0000\n",
            "Epoch 137: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 2.7103 - val_accuracy: 0.8500\n",
            "Epoch 138/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0502 - accuracy: 1.0000\n",
            "Epoch 138: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.9875 - val_loss: 2.6630 - val_accuracy: 0.8500\n",
            "Epoch 139/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9667\n",
            "Epoch 139: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0466 - accuracy: 0.9812 - val_loss: 2.6545 - val_accuracy: 0.8750\n",
            "Epoch 140/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2575 - accuracy: 0.9000\n",
            "Epoch 140: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9563 - val_loss: 2.6026 - val_accuracy: 0.9000\n",
            "Epoch 141/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 141: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0626 - accuracy: 0.9875 - val_loss: 2.5916 - val_accuracy: 0.9000\n",
            "Epoch 142/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0249 - accuracy: 1.0000\n",
            "Epoch 142: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0829 - accuracy: 0.9750 - val_loss: 2.6415 - val_accuracy: 0.8750\n",
            "Epoch 143/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0943 - accuracy: 0.9333\n",
            "Epoch 143: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0690 - accuracy: 0.9688 - val_loss: 2.6793 - val_accuracy: 0.9000\n",
            "Epoch 144/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 144: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0213 - accuracy: 0.9937 - val_loss: 2.7426 - val_accuracy: 0.8750\n",
            "Epoch 145/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1046 - accuracy: 0.9333\n",
            "Epoch 145: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0405 - accuracy: 0.9812 - val_loss: 2.7695 - val_accuracy: 0.8500\n",
            "Epoch 146/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1187 - accuracy: 0.9000\n",
            "Epoch 146: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0649 - accuracy: 0.9688 - val_loss: 2.7635 - val_accuracy: 0.8750\n",
            "Epoch 147/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 147: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0936 - accuracy: 0.9750 - val_loss: 2.7457 - val_accuracy: 0.8750\n",
            "Epoch 148/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 148: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0452 - accuracy: 0.9875 - val_loss: 2.7362 - val_accuracy: 0.8750\n",
            "Epoch 149/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1067 - accuracy: 0.9667\n",
            "Epoch 149: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1871 - accuracy: 0.9688 - val_loss: 2.7534 - val_accuracy: 0.8750\n",
            "Epoch 150/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0772 - accuracy: 0.9667\n",
            "Epoch 150: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0448 - accuracy: 0.9812 - val_loss: 2.7818 - val_accuracy: 0.8500\n",
            "Epoch 151/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 151: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0503 - accuracy: 0.9937 - val_loss: 2.8288 - val_accuracy: 0.8500\n",
            "Epoch 152/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0182 - accuracy: 1.0000\n",
            "Epoch 152: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1027 - accuracy: 0.9750 - val_loss: 2.8250 - val_accuracy: 0.8500\n",
            "Epoch 153/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 153: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0847 - accuracy: 0.9812 - val_loss: 2.8467 - val_accuracy: 0.8250\n",
            "Epoch 154/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 154: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0837 - accuracy: 0.9812 - val_loss: 3.0208 - val_accuracy: 0.8250\n",
            "Epoch 155/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 155: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 3.1164 - val_accuracy: 0.8250\n",
            "Epoch 156/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 156: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1553 - accuracy: 0.9688 - val_loss: 2.8668 - val_accuracy: 0.9000\n",
            "Epoch 157/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0982 - accuracy: 0.9667\n",
            "Epoch 157: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0597 - accuracy: 0.9812 - val_loss: 2.6442 - val_accuracy: 0.9000\n",
            "Epoch 158/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 158: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0754 - accuracy: 0.9688 - val_loss: 2.5700 - val_accuracy: 0.9000\n",
            "Epoch 159/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 159: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0558 - accuracy: 0.9812 - val_loss: 2.5489 - val_accuracy: 0.9000\n",
            "Epoch 160/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 160: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0431 - accuracy: 0.9937 - val_loss: 2.5666 - val_accuracy: 0.9000\n",
            "Epoch 161/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9667\n",
            "Epoch 161: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 2.6052 - val_accuracy: 0.9000\n",
            "Epoch 162/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1824 - accuracy: 0.9333\n",
            "Epoch 162: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0622 - accuracy: 0.9812 - val_loss: 2.6182 - val_accuracy: 0.9000\n",
            "Epoch 163/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0657 - accuracy: 0.9667\n",
            "Epoch 163: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0504 - accuracy: 0.9812 - val_loss: 2.6361 - val_accuracy: 0.9000\n",
            "Epoch 164/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0251 - accuracy: 1.0000\n",
            "Epoch 164: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 2.6519 - val_accuracy: 0.9000\n",
            "Epoch 165/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 165: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0611 - accuracy: 0.9812 - val_loss: 2.6695 - val_accuracy: 0.9000\n",
            "Epoch 166/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 166: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0343 - accuracy: 0.9937 - val_loss: 2.6986 - val_accuracy: 0.9000\n",
            "Epoch 167/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1532 - accuracy: 0.9667\n",
            "Epoch 167: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0570 - accuracy: 0.9875 - val_loss: 2.7240 - val_accuracy: 0.9000\n",
            "Epoch 168/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 168: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0629 - accuracy: 0.9812 - val_loss: 2.7902 - val_accuracy: 0.9000\n",
            "Epoch 169/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 169: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0470 - accuracy: 0.9812 - val_loss: 2.8363 - val_accuracy: 0.9000\n",
            "Epoch 170/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 170: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0472 - accuracy: 0.9812 - val_loss: 2.8984 - val_accuracy: 0.8750\n",
            "Epoch 171/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 171: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 2.9402 - val_accuracy: 0.8750\n",
            "Epoch 172/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 172: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0443 - accuracy: 0.9875 - val_loss: 2.9627 - val_accuracy: 0.8500\n",
            "Epoch 173/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 173: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0392 - accuracy: 0.9812 - val_loss: 2.9261 - val_accuracy: 0.9000\n",
            "Epoch 174/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9667\n",
            "Epoch 174: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0465 - accuracy: 0.9750 - val_loss: 2.8769 - val_accuracy: 0.9000\n",
            "Epoch 175/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0588 - accuracy: 0.9667\n",
            "Epoch 175: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9750 - val_loss: 2.8842 - val_accuracy: 0.9000\n",
            "Epoch 176/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2459 - accuracy: 0.9333\n",
            "Epoch 176: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0746 - accuracy: 0.9812 - val_loss: 2.9108 - val_accuracy: 0.9000\n",
            "Epoch 177/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0843 - accuracy: 0.9667\n",
            "Epoch 177: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0929 - accuracy: 0.9625 - val_loss: 2.9269 - val_accuracy: 0.9000\n",
            "Epoch 178/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0852 - accuracy: 1.0000\n",
            "Epoch 178: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0587 - accuracy: 0.9875 - val_loss: 2.9229 - val_accuracy: 0.9000\n",
            "Epoch 179/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 179: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0287 - accuracy: 0.9937 - val_loss: 2.9169 - val_accuracy: 0.9000\n",
            "Epoch 180/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1766 - accuracy: 0.9333\n",
            "Epoch 180: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0707 - accuracy: 0.9750 - val_loss: 2.8855 - val_accuracy: 0.9000\n",
            "Epoch 181/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 8.5058e-04 - accuracy: 1.0000\n",
            "Epoch 181: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0357 - accuracy: 0.9875 - val_loss: 2.8420 - val_accuracy: 0.8750\n",
            "Epoch 182/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0886 - accuracy: 0.9667\n",
            "Epoch 182: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0304 - accuracy: 0.9875 - val_loss: 2.8518 - val_accuracy: 0.8750\n",
            "Epoch 183/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0160 - accuracy: 1.0000\n",
            "Epoch 183: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0452 - accuracy: 0.9875 - val_loss: 2.9429 - val_accuracy: 0.8750\n",
            "Epoch 184/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0851 - accuracy: 0.9667\n",
            "Epoch 184: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0731 - accuracy: 0.9750 - val_loss: 3.1121 - val_accuracy: 0.8750\n",
            "Epoch 185/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0580 - accuracy: 0.9667\n",
            "Epoch 185: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 3.1575 - val_accuracy: 0.8500\n",
            "Epoch 186/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9667\n",
            "Epoch 186: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0695 - accuracy: 0.9688 - val_loss: 3.1579 - val_accuracy: 0.8500\n",
            "Epoch 187/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1734 - accuracy: 0.9333\n",
            "Epoch 187: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 3.0552 - val_accuracy: 0.8500\n",
            "Epoch 188/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0614 - accuracy: 0.9667\n",
            "Epoch 188: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0401 - accuracy: 0.9812 - val_loss: 3.0015 - val_accuracy: 0.8750\n",
            "Epoch 189/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.4423 - accuracy: 0.9000\n",
            "Epoch 189: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2297 - accuracy: 0.9312 - val_loss: 2.9895 - val_accuracy: 0.9000\n",
            "Epoch 190/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0592 - accuracy: 0.9667\n",
            "Epoch 190: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0418 - accuracy: 0.9875 - val_loss: 2.9943 - val_accuracy: 0.9000\n",
            "Epoch 191/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.3333 - accuracy: 0.9000\n",
            "Epoch 191: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0906 - accuracy: 0.9750 - val_loss: 2.9955 - val_accuracy: 0.9000\n",
            "Epoch 192/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 192: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0269 - accuracy: 0.9875 - val_loss: 2.9632 - val_accuracy: 0.9000\n",
            "Epoch 193/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 193: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0593 - accuracy: 0.9750 - val_loss: 2.9274 - val_accuracy: 0.9000\n",
            "Epoch 194/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 194: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0452 - accuracy: 0.9875 - val_loss: 2.9004 - val_accuracy: 0.9000\n",
            "Epoch 195/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0872 - accuracy: 0.9667\n",
            "Epoch 195: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1281 - accuracy: 0.9500 - val_loss: 2.8922 - val_accuracy: 0.9000\n",
            "Epoch 196/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 7.2432e-04 - accuracy: 1.0000\n",
            "Epoch 196: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0546 - accuracy: 0.9812 - val_loss: 2.9089 - val_accuracy: 0.8750\n",
            "Epoch 197/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000\n",
            "Epoch 197: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0773 - accuracy: 0.9750 - val_loss: 3.0565 - val_accuracy: 0.8500\n",
            "Epoch 198/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2345 - accuracy: 0.9333\n",
            "Epoch 198: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1304 - accuracy: 0.9688 - val_loss: 3.0679 - val_accuracy: 0.8250\n",
            "Epoch 199/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 199: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0873 - accuracy: 0.9750 - val_loss: 2.8756 - val_accuracy: 0.8750\n",
            "Epoch 200/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 200: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 2.7288 - val_accuracy: 0.9000\n",
            "Epoch 201/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 201: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0906 - accuracy: 0.9750 - val_loss: 2.6858 - val_accuracy: 0.9000\n",
            "Epoch 202/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0604 - accuracy: 1.0000\n",
            "Epoch 202: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0879 - accuracy: 0.9750 - val_loss: 2.7116 - val_accuracy: 0.9000\n",
            "Epoch 203/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9667\n",
            "Epoch 203: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0440 - accuracy: 0.9812 - val_loss: 2.8372 - val_accuracy: 0.9000\n",
            "Epoch 204/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 204: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0557 - accuracy: 0.9812 - val_loss: 2.9100 - val_accuracy: 0.9000\n",
            "Epoch 205/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1188 - accuracy: 0.9333\n",
            "Epoch 205: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0914 - accuracy: 0.9688 - val_loss: 2.9382 - val_accuracy: 0.8750\n",
            "Epoch 206/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9667\n",
            "Epoch 206: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0908 - accuracy: 0.9812 - val_loss: 2.9270 - val_accuracy: 0.8750\n",
            "Epoch 207/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0460 - accuracy: 0.9667\n",
            "Epoch 207: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1403 - accuracy: 0.9750 - val_loss: 2.9477 - val_accuracy: 0.8750\n",
            "Epoch 208/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0273 - accuracy: 1.0000\n",
            "Epoch 208: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0430 - accuracy: 0.9937 - val_loss: 2.9857 - val_accuracy: 0.9000\n",
            "Epoch 209/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 209: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0261 - accuracy: 0.9937 - val_loss: 3.0014 - val_accuracy: 0.9000\n",
            "Epoch 210/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0702 - accuracy: 0.9667\n",
            "Epoch 210: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0431 - accuracy: 0.9812 - val_loss: 3.0105 - val_accuracy: 0.9000\n",
            "Epoch 211/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0700 - accuracy: 1.0000\n",
            "Epoch 211: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 3.0381 - val_accuracy: 0.8750\n",
            "Epoch 212/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 212: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0378 - accuracy: 0.9812 - val_loss: 3.0842 - val_accuracy: 0.8750\n",
            "Epoch 213/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9667\n",
            "Epoch 213: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9937 - val_loss: 3.1446 - val_accuracy: 0.8750\n",
            "Epoch 214/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 214: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0663 - accuracy: 0.9812 - val_loss: 3.2010 - val_accuracy: 0.8750\n",
            "Epoch 215/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2082 - accuracy: 0.9667\n",
            "Epoch 215: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0741 - accuracy: 0.9812 - val_loss: 3.0540 - val_accuracy: 0.9000\n",
            "Epoch 216/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 216: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0272 - accuracy: 0.9937 - val_loss: 3.0466 - val_accuracy: 0.9000\n",
            "Epoch 217/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1206 - accuracy: 0.9667\n",
            "Epoch 217: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0675 - accuracy: 0.9750 - val_loss: 3.0755 - val_accuracy: 0.9000\n",
            "Epoch 218/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 218: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0163 - accuracy: 0.9937 - val_loss: 3.1742 - val_accuracy: 0.9000\n",
            "Epoch 219/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0519 - accuracy: 0.9667\n",
            "Epoch 219: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0803 - accuracy: 0.9750 - val_loss: 3.2866 - val_accuracy: 0.8750\n",
            "Epoch 220/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 220: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0274 - accuracy: 0.9937 - val_loss: 3.2767 - val_accuracy: 0.8750\n",
            "Epoch 221/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 221: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0338 - accuracy: 0.9875 - val_loss: 3.2520 - val_accuracy: 0.8750\n",
            "Epoch 222/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 222: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0498 - accuracy: 0.9812 - val_loss: 3.1929 - val_accuracy: 0.9000\n",
            "Epoch 223/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 223: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0498 - accuracy: 0.9875 - val_loss: 3.1698 - val_accuracy: 0.9000\n",
            "Epoch 224/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 224: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0365 - accuracy: 0.9875 - val_loss: 3.2252 - val_accuracy: 0.9000\n",
            "Epoch 225/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 225: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0391 - accuracy: 0.9875 - val_loss: 3.2743 - val_accuracy: 0.9000\n",
            "Epoch 226/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2006 - accuracy: 0.9000\n",
            "Epoch 226: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0768 - accuracy: 0.9625 - val_loss: 3.3214 - val_accuracy: 0.8750\n",
            "Epoch 227/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 227: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0495 - accuracy: 0.9812 - val_loss: 3.3733 - val_accuracy: 0.8500\n",
            "Epoch 228/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1109 - accuracy: 0.9667\n",
            "Epoch 228: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0733 - accuracy: 0.9812 - val_loss: 3.3841 - val_accuracy: 0.8500\n",
            "Epoch 229/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2197 - accuracy: 0.9667\n",
            "Epoch 229: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0702 - accuracy: 0.9875 - val_loss: 3.3452 - val_accuracy: 0.8500\n",
            "Epoch 230/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9667\n",
            "Epoch 230: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 0.9875 - val_loss: 3.4452 - val_accuracy: 0.8500\n",
            "Epoch 231/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 231: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0286 - accuracy: 0.9937 - val_loss: 3.5114 - val_accuracy: 0.8500\n",
            "Epoch 232/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9667\n",
            "Epoch 232: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0281 - accuracy: 0.9875 - val_loss: 3.5662 - val_accuracy: 0.8500\n",
            "Epoch 233/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 233: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 3.6098 - val_accuracy: 0.8500\n",
            "Epoch 234/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1620 - accuracy: 0.9333\n",
            "Epoch 234: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0721 - accuracy: 0.9750 - val_loss: 3.3827 - val_accuracy: 0.8500\n",
            "Epoch 235/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1447 - accuracy: 0.9333\n",
            "Epoch 235: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 3.1724 - val_accuracy: 0.8500\n",
            "Epoch 236/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.4425 - accuracy: 0.9000\n",
            "Epoch 236: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1363 - accuracy: 0.9563 - val_loss: 3.0902 - val_accuracy: 0.8500\n",
            "Epoch 237/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0230 - accuracy: 1.0000\n",
            "Epoch 237: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0790 - accuracy: 0.9937 - val_loss: 3.0707 - val_accuracy: 0.8500\n",
            "Epoch 238/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 238: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.0850 - val_accuracy: 0.8750\n",
            "Epoch 239/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1307 - accuracy: 0.9667\n",
            "Epoch 239: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 3.1055 - val_accuracy: 0.8750\n",
            "Epoch 240/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0822 - accuracy: 0.9667\n",
            "Epoch 240: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 3.1472 - val_accuracy: 0.8750\n",
            "Epoch 241/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 241: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 3.1952 - val_accuracy: 0.8750\n",
            "Epoch 242/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0238 - accuracy: 1.0000\n",
            "Epoch 242: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 3.2456 - val_accuracy: 0.9000\n",
            "Epoch 243/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 243: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0144 - accuracy: 0.9937 - val_loss: 3.3069 - val_accuracy: 0.9000\n",
            "Epoch 244/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 244: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.3668 - val_accuracy: 0.9000\n",
            "Epoch 245/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 245: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0305 - accuracy: 0.9937 - val_loss: 3.4764 - val_accuracy: 0.8750\n",
            "Epoch 246/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 246: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0431 - accuracy: 0.9812 - val_loss: 3.5969 - val_accuracy: 0.8500\n",
            "Epoch 247/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9667\n",
            "Epoch 247: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0285 - accuracy: 0.9937 - val_loss: 3.6873 - val_accuracy: 0.8500\n",
            "Epoch 248/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1979 - accuracy: 0.9667\n",
            "Epoch 248: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0860 - accuracy: 0.9688 - val_loss: 3.6032 - val_accuracy: 0.8500\n",
            "Epoch 249/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 249: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0285 - accuracy: 0.9937 - val_loss: 3.5671 - val_accuracy: 0.8750\n",
            "Epoch 250/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 250: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0745 - accuracy: 0.9750 - val_loss: 3.5381 - val_accuracy: 0.8750\n",
            "Epoch 251/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0998 - accuracy: 0.9667\n",
            "Epoch 251: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 3.5435 - val_accuracy: 0.8750\n",
            "Epoch 252/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 252: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 3.5604 - val_accuracy: 0.8500\n",
            "Epoch 253/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0924 - accuracy: 0.9667\n",
            "Epoch 253: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0819 - accuracy: 0.9750 - val_loss: 3.6739 - val_accuracy: 0.8750\n",
            "Epoch 254/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2064 - accuracy: 0.9667\n",
            "Epoch 254: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0734 - accuracy: 0.9812 - val_loss: 3.7528 - val_accuracy: 0.8500\n",
            "Epoch 255/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000\n",
            "Epoch 255: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0422 - accuracy: 0.9875 - val_loss: 3.7011 - val_accuracy: 0.8250\n",
            "Epoch 256/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 256: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0333 - accuracy: 0.9812 - val_loss: 3.4717 - val_accuracy: 0.8500\n",
            "Epoch 257/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1224 - accuracy: 0.9667\n",
            "Epoch 257: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1456 - accuracy: 0.9688 - val_loss: 3.2305 - val_accuracy: 0.8750\n",
            "Epoch 258/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 3.9320e-05 - accuracy: 1.0000\n",
            "Epoch 258: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0477 - accuracy: 0.9937 - val_loss: 3.1225 - val_accuracy: 0.8500\n",
            "Epoch 259/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1450 - accuracy: 0.9333\n",
            "Epoch 259: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0831 - accuracy: 0.9625 - val_loss: 3.0809 - val_accuracy: 0.8250\n",
            "Epoch 260/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 260: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0740 - accuracy: 0.9750 - val_loss: 3.0700 - val_accuracy: 0.8500\n",
            "Epoch 261/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 261: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0650 - accuracy: 0.9875 - val_loss: 3.0009 - val_accuracy: 0.8750\n",
            "Epoch 262/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0714 - accuracy: 0.9667\n",
            "Epoch 262: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0884 - accuracy: 0.9563 - val_loss: 3.0040 - val_accuracy: 0.9000\n",
            "Epoch 263/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 263: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 3.0413 - val_accuracy: 0.8750\n",
            "Epoch 264/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0726 - accuracy: 0.9667\n",
            "Epoch 264: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0404 - accuracy: 0.9812 - val_loss: 3.0854 - val_accuracy: 0.8750\n",
            "Epoch 265/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0184 - accuracy: 1.0000\n",
            "Epoch 265: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0397 - accuracy: 0.9937 - val_loss: 3.1281 - val_accuracy: 0.8500\n",
            "Epoch 266/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0359 - accuracy: 0.9667\n",
            "Epoch 266: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0347 - accuracy: 0.9875 - val_loss: 3.2988 - val_accuracy: 0.8250\n",
            "Epoch 267/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 267: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0551 - accuracy: 0.9688 - val_loss: 3.4997 - val_accuracy: 0.8250\n",
            "Epoch 268/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 268: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0378 - accuracy: 0.9812 - val_loss: 3.4094 - val_accuracy: 0.8750\n",
            "Epoch 269/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1186 - accuracy: 0.9667\n",
            "Epoch 269: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0712 - accuracy: 0.9750 - val_loss: 3.3114 - val_accuracy: 0.8750\n",
            "Epoch 270/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
            "Epoch 270: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0286 - accuracy: 0.9937 - val_loss: 3.2435 - val_accuracy: 0.8500\n",
            "Epoch 271/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 271: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.0459 - accuracy: 0.9875 - val_loss: 3.2232 - val_accuracy: 0.8500\n",
            "Epoch 272/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0767 - accuracy: 0.9333\n",
            "Epoch 272: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0678 - accuracy: 0.9688 - val_loss: 3.1927 - val_accuracy: 0.8750\n",
            "Epoch 273/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 273: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 3.2358 - val_accuracy: 0.8500\n",
            "Epoch 274/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 274: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0292 - accuracy: 0.9875 - val_loss: 3.2985 - val_accuracy: 0.8500\n",
            "Epoch 275/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 275: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0957 - accuracy: 0.9688 - val_loss: 3.3716 - val_accuracy: 0.8250\n",
            "Epoch 276/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1375 - accuracy: 0.9667\n",
            "Epoch 276: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0563 - accuracy: 0.9875 - val_loss: 3.1999 - val_accuracy: 0.8500\n",
            "Epoch 277/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0248 - accuracy: 1.0000\n",
            "Epoch 277: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0805 - accuracy: 0.9812 - val_loss: 3.1393 - val_accuracy: 0.8500\n",
            "Epoch 278/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1316 - accuracy: 0.9333\n",
            "Epoch 278: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0843 - accuracy: 0.9625 - val_loss: 3.1017 - val_accuracy: 0.8500\n",
            "Epoch 279/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1905 - accuracy: 0.9333\n",
            "Epoch 279: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0750 - accuracy: 0.9688 - val_loss: 3.0643 - val_accuracy: 0.8750\n",
            "Epoch 280/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 280: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0652 - accuracy: 0.9875 - val_loss: 2.9719 - val_accuracy: 0.8750\n",
            "Epoch 281/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 281: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0291 - accuracy: 0.9875 - val_loss: 2.8817 - val_accuracy: 0.8750\n",
            "Epoch 282/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1022 - accuracy: 0.9333\n",
            "Epoch 282: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0696 - accuracy: 0.9625 - val_loss: 2.8714 - val_accuracy: 0.8750\n",
            "Epoch 283/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1131 - accuracy: 0.9667\n",
            "Epoch 283: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0561 - accuracy: 0.9875 - val_loss: 2.8612 - val_accuracy: 0.8500\n",
            "Epoch 284/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0160 - accuracy: 1.0000\n",
            "Epoch 284: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 2.9129 - val_accuracy: 0.8500\n",
            "Epoch 285/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0244 - accuracy: 1.0000\n",
            "Epoch 285: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0247 - accuracy: 0.9875 - val_loss: 2.9494 - val_accuracy: 0.8500\n",
            "Epoch 286/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 286: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0894 - accuracy: 0.9688 - val_loss: 3.0333 - val_accuracy: 0.8750\n",
            "Epoch 287/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 287: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0491 - accuracy: 0.9875 - val_loss: 3.0260 - val_accuracy: 0.8750\n",
            "Epoch 288/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 288: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0332 - accuracy: 0.9875 - val_loss: 2.9183 - val_accuracy: 0.8750\n",
            "Epoch 289/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0837 - accuracy: 0.9667\n",
            "Epoch 289: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0886 - accuracy: 0.9688 - val_loss: 2.8330 - val_accuracy: 0.9000\n",
            "Epoch 290/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0762 - accuracy: 0.9667\n",
            "Epoch 290: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0379 - accuracy: 0.9875 - val_loss: 2.7285 - val_accuracy: 0.9000\n",
            "Epoch 291/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0435 - accuracy: 1.0000\n",
            "Epoch 291: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0345 - accuracy: 0.9937 - val_loss: 2.6983 - val_accuracy: 0.9000\n",
            "Epoch 292/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
            "Epoch 292: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0823 - accuracy: 0.9750 - val_loss: 2.7174 - val_accuracy: 0.9000\n",
            "Epoch 293/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1745 - accuracy: 0.9667\n",
            "Epoch 293: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0908 - accuracy: 0.9750 - val_loss: 2.8127 - val_accuracy: 0.8750\n",
            "Epoch 294/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1144 - accuracy: 0.9333\n",
            "Epoch 294: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0855 - accuracy: 0.9750 - val_loss: 2.9031 - val_accuracy: 0.8500\n",
            "Epoch 295/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1866 - accuracy: 0.9667\n",
            "Epoch 295: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0560 - accuracy: 0.9875 - val_loss: 2.9479 - val_accuracy: 0.8500\n",
            "Epoch 296/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 296: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0647 - accuracy: 0.9688 - val_loss: 3.0375 - val_accuracy: 0.8500\n",
            "Epoch 297/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 297: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0926 - accuracy: 0.9688 - val_loss: 2.9912 - val_accuracy: 0.8750\n",
            "Epoch 298/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0936 - accuracy: 0.9667\n",
            "Epoch 298: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0352 - accuracy: 0.9937 - val_loss: 2.9795 - val_accuracy: 0.8750\n",
            "Epoch 299/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 5.9141e-04 - accuracy: 1.0000\n",
            "Epoch 299: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0969 - accuracy: 0.9812 - val_loss: 2.9449 - val_accuracy: 0.8750\n",
            "Epoch 300/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0718 - accuracy: 0.9667\n",
            "Epoch 300: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0728 - accuracy: 0.9750 - val_loss: 2.8919 - val_accuracy: 0.9000\n",
            "Epoch 301/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 301: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0564 - accuracy: 0.9750 - val_loss: 2.8832 - val_accuracy: 0.9000\n",
            "Epoch 302/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9667\n",
            "Epoch 302: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0333 - accuracy: 0.9875 - val_loss: 2.9857 - val_accuracy: 0.9000\n",
            "Epoch 303/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 5.0146e-04 - accuracy: 1.0000\n",
            "Epoch 303: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0364 - accuracy: 0.9937 - val_loss: 3.0506 - val_accuracy: 0.8750\n",
            "Epoch 304/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
            "Epoch 304: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0507 - accuracy: 0.9875 - val_loss: 3.1054 - val_accuracy: 0.8750\n",
            "Epoch 305/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0732 - accuracy: 0.9667\n",
            "Epoch 305: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0490 - accuracy: 0.9750 - val_loss: 3.1342 - val_accuracy: 0.8750\n",
            "Epoch 306/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0666 - accuracy: 0.9667\n",
            "Epoch 306: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0666 - accuracy: 0.9750 - val_loss: 3.1794 - val_accuracy: 0.8750\n",
            "Epoch 307/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 307: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 3.2220 - val_accuracy: 0.8750\n",
            "Epoch 308/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0425 - accuracy: 1.0000\n",
            "Epoch 308: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0669 - accuracy: 0.9812 - val_loss: 3.2642 - val_accuracy: 0.8750\n",
            "Epoch 309/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 309: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0756 - accuracy: 0.9750 - val_loss: 3.2429 - val_accuracy: 0.8750\n",
            "Epoch 310/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0545 - accuracy: 1.0000\n",
            "Epoch 310: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0333 - accuracy: 0.9937 - val_loss: 3.1912 - val_accuracy: 0.9000\n",
            "Epoch 311/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 311: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 3.1844 - val_accuracy: 0.9000\n",
            "Epoch 312/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 312: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0436 - accuracy: 0.9875 - val_loss: 3.2906 - val_accuracy: 0.9000\n",
            "Epoch 313/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 313: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0340 - accuracy: 0.9812 - val_loss: 3.4049 - val_accuracy: 0.9000\n",
            "Epoch 314/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 3.4284e-04 - accuracy: 1.0000\n",
            "Epoch 314: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 3.4341 - val_accuracy: 0.9000\n",
            "Epoch 315/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 315: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0423 - accuracy: 0.9875 - val_loss: 3.4380 - val_accuracy: 0.9000\n",
            "Epoch 316/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 316: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.4555 - val_accuracy: 0.9000\n",
            "Epoch 317/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 317: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 3.4983 - val_accuracy: 0.9000\n",
            "Epoch 318/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000\n",
            "Epoch 318: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0414 - accuracy: 0.9875 - val_loss: 3.4402 - val_accuracy: 0.9000\n",
            "Epoch 319/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 8.3928e-04 - accuracy: 1.0000\n",
            "Epoch 319: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0145 - accuracy: 0.9937 - val_loss: 3.3932 - val_accuracy: 0.9000\n",
            "Epoch 320/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 320: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 3.3883 - val_accuracy: 0.9000\n",
            "Epoch 321/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 321: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0446 - accuracy: 0.9812 - val_loss: 3.3985 - val_accuracy: 0.9000\n",
            "Epoch 322/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0197 - accuracy: 1.0000\n",
            "Epoch 322: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0938 - accuracy: 0.9750 - val_loss: 3.4367 - val_accuracy: 0.8750\n",
            "Epoch 323/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 6.8794e-04 - accuracy: 1.0000\n",
            "Epoch 323: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0593 - accuracy: 0.9875 - val_loss: 3.5028 - val_accuracy: 0.8750\n",
            "Epoch 324/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2528 - accuracy: 0.9333\n",
            "Epoch 324: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1781 - accuracy: 0.9500 - val_loss: 3.4996 - val_accuracy: 0.9000\n",
            "Epoch 325/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.3149 - accuracy: 0.9333\n",
            "Epoch 325: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0995 - accuracy: 0.9750 - val_loss: 3.5712 - val_accuracy: 0.8250\n",
            "Epoch 326/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 5.6475e-04 - accuracy: 1.0000\n",
            "Epoch 326: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0251 - accuracy: 0.9937 - val_loss: 3.7354 - val_accuracy: 0.8250\n",
            "Epoch 327/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0568 - accuracy: 0.9667\n",
            "Epoch 327: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0519 - accuracy: 0.9812 - val_loss: 3.7710 - val_accuracy: 0.8250\n",
            "Epoch 328/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0199 - accuracy: 1.0000\n",
            "Epoch 328: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0766 - accuracy: 0.9875 - val_loss: 3.5980 - val_accuracy: 0.8250\n",
            "Epoch 329/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 329: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1011 - accuracy: 0.9688 - val_loss: 3.3896 - val_accuracy: 0.8250\n",
            "Epoch 330/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 330: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0156 - accuracy: 0.9937 - val_loss: 3.2194 - val_accuracy: 0.8500\n",
            "Epoch 331/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0234 - accuracy: 1.0000\n",
            "Epoch 331: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0634 - accuracy: 0.9688 - val_loss: 3.0048 - val_accuracy: 0.8500\n",
            "Epoch 332/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2051 - accuracy: 0.9333\n",
            "Epoch 332: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1433 - accuracy: 0.9563 - val_loss: 2.7745 - val_accuracy: 0.9000\n",
            "Epoch 333/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 2.8786e-05 - accuracy: 1.0000\n",
            "Epoch 333: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0453 - accuracy: 0.9812 - val_loss: 2.5183 - val_accuracy: 0.9000\n",
            "Epoch 334/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0657 - accuracy: 0.9667\n",
            "Epoch 334: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1579 - accuracy: 0.9625 - val_loss: 2.4097 - val_accuracy: 0.9000\n",
            "Epoch 335/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9667\n",
            "Epoch 335: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0905 - accuracy: 0.9625 - val_loss: 2.3195 - val_accuracy: 0.9000\n",
            "Epoch 336/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1005 - accuracy: 0.9333\n",
            "Epoch 336: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1547 - accuracy: 0.9625 - val_loss: 2.2767 - val_accuracy: 0.9000\n",
            "Epoch 337/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0276 - accuracy: 1.0000\n",
            "Epoch 337: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0529 - accuracy: 0.9937 - val_loss: 2.2895 - val_accuracy: 0.8750\n",
            "Epoch 338/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 338: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0434 - accuracy: 0.9937 - val_loss: 2.4110 - val_accuracy: 0.9000\n",
            "Epoch 339/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1183 - accuracy: 0.9667\n",
            "Epoch 339: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0357 - accuracy: 0.9937 - val_loss: 2.5233 - val_accuracy: 0.9000\n",
            "Epoch 340/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0177 - accuracy: 1.0000\n",
            "Epoch 340: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0370 - accuracy: 0.9812 - val_loss: 2.6301 - val_accuracy: 0.9000\n",
            "Epoch 341/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0798 - accuracy: 0.9667\n",
            "Epoch 341: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 2.7180 - val_accuracy: 0.9000\n",
            "Epoch 342/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0222 - accuracy: 1.0000\n",
            "Epoch 342: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0482 - accuracy: 0.9875 - val_loss: 2.7706 - val_accuracy: 0.9000\n",
            "Epoch 343/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0233 - accuracy: 1.0000\n",
            "Epoch 343: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 2.8156 - val_accuracy: 0.8750\n",
            "Epoch 344/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0451 - accuracy: 0.9667\n",
            "Epoch 344: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0408 - accuracy: 0.9875 - val_loss: 2.8504 - val_accuracy: 0.8750\n",
            "Epoch 345/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 345: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0361 - accuracy: 0.9937 - val_loss: 2.8567 - val_accuracy: 0.8750\n",
            "Epoch 346/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0770 - accuracy: 0.9667\n",
            "Epoch 346: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0763 - accuracy: 0.9750 - val_loss: 2.8705 - val_accuracy: 0.8750\n",
            "Epoch 347/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 347: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 2.9522 - val_accuracy: 0.8750\n",
            "Epoch 348/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0900 - accuracy: 0.9667\n",
            "Epoch 348: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0942 - accuracy: 0.9750 - val_loss: 2.9622 - val_accuracy: 0.8750\n",
            "Epoch 349/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0819 - accuracy: 0.9667\n",
            "Epoch 349: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0367 - accuracy: 0.9875 - val_loss: 3.0046 - val_accuracy: 0.8750\n",
            "Epoch 350/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 350: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0307 - accuracy: 0.9875 - val_loss: 3.0572 - val_accuracy: 0.8750\n",
            "Epoch 351/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 351: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0536 - accuracy: 0.9812 - val_loss: 3.1407 - val_accuracy: 0.8500\n",
            "Epoch 352/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 352: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0971 - accuracy: 0.9750 - val_loss: 3.1881 - val_accuracy: 0.8250\n",
            "Epoch 353/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0244 - accuracy: 1.0000\n",
            "Epoch 353: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0511 - accuracy: 0.9750 - val_loss: 2.9179 - val_accuracy: 0.8250\n",
            "Epoch 354/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1545 - accuracy: 0.9667\n",
            "Epoch 354: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1593 - accuracy: 0.9688 - val_loss: 2.5204 - val_accuracy: 0.8750\n",
            "Epoch 355/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 355: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0646 - accuracy: 0.9688 - val_loss: 2.3954 - val_accuracy: 0.9000\n",
            "Epoch 356/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 356: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1076 - accuracy: 0.9688 - val_loss: 2.4390 - val_accuracy: 0.9000\n",
            "Epoch 357/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2151 - accuracy: 0.9333\n",
            "Epoch 357: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1081 - accuracy: 0.9625 - val_loss: 2.5953 - val_accuracy: 0.9000\n",
            "Epoch 358/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1132 - accuracy: 0.9667\n",
            "Epoch 358: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0847 - accuracy: 0.9625 - val_loss: 2.6434 - val_accuracy: 0.8750\n",
            "Epoch 359/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 359: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0796 - accuracy: 0.9688 - val_loss: 2.6762 - val_accuracy: 0.8750\n",
            "Epoch 360/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000\n",
            "Epoch 360: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0784 - accuracy: 0.9812 - val_loss: 2.6895 - val_accuracy: 0.8750\n",
            "Epoch 361/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 361: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0434 - accuracy: 0.9812 - val_loss: 2.7011 - val_accuracy: 0.8750\n",
            "Epoch 362/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0199 - accuracy: 1.0000\n",
            "Epoch 362: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 2.7197 - val_accuracy: 0.8750\n",
            "Epoch 363/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 363: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0677 - accuracy: 0.9812 - val_loss: 2.7646 - val_accuracy: 0.8750\n",
            "Epoch 364/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 9.0698e-04 - accuracy: 1.0000\n",
            "Epoch 364: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0264 - accuracy: 0.9937 - val_loss: 2.8281 - val_accuracy: 0.8750\n",
            "Epoch 365/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9333\n",
            "Epoch 365: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0309 - accuracy: 0.9875 - val_loss: 2.8820 - val_accuracy: 0.8750\n",
            "Epoch 366/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 366: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0246 - accuracy: 0.9875 - val_loss: 2.9299 - val_accuracy: 0.8750\n",
            "Epoch 367/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 367: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0269 - accuracy: 0.9937 - val_loss: 2.9691 - val_accuracy: 0.8750\n",
            "Epoch 368/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1302 - accuracy: 0.9667\n",
            "Epoch 368: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0336 - accuracy: 0.9875 - val_loss: 2.9490 - val_accuracy: 0.8750\n",
            "Epoch 369/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1324 - accuracy: 0.9667\n",
            "Epoch 369: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0393 - accuracy: 0.9937 - val_loss: 2.9467 - val_accuracy: 0.8750\n",
            "Epoch 370/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0571 - accuracy: 0.9667\n",
            "Epoch 370: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0421 - accuracy: 0.9812 - val_loss: 2.9773 - val_accuracy: 0.8750\n",
            "Epoch 371/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0345 - accuracy: 0.9667\n",
            "Epoch 371: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0456 - accuracy: 0.9875 - val_loss: 3.0468 - val_accuracy: 0.8750\n",
            "Epoch 372/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 372: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0938 - accuracy: 0.9812 - val_loss: 3.1042 - val_accuracy: 0.8750\n",
            "Epoch 373/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0941 - accuracy: 0.9667\n",
            "Epoch 373: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0528 - accuracy: 0.9875 - val_loss: 3.1247 - val_accuracy: 0.8500\n",
            "Epoch 374/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 374: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0269 - accuracy: 0.9937 - val_loss: 3.1313 - val_accuracy: 0.8500\n",
            "Epoch 375/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0211 - accuracy: 1.0000\n",
            "Epoch 375: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0641 - accuracy: 0.9812 - val_loss: 3.1646 - val_accuracy: 0.8250\n",
            "Epoch 376/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1901 - accuracy: 0.9667\n",
            "Epoch 376: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0620 - accuracy: 0.9875 - val_loss: 3.1784 - val_accuracy: 0.8250\n",
            "Epoch 377/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 377: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0365 - accuracy: 0.9875 - val_loss: 3.1931 - val_accuracy: 0.8250\n",
            "Epoch 378/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0640 - accuracy: 0.9667\n",
            "Epoch 378: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0603 - accuracy: 0.9750 - val_loss: 3.1911 - val_accuracy: 0.8250\n",
            "Epoch 379/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 379: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0815 - accuracy: 0.9812 - val_loss: 3.0367 - val_accuracy: 0.8000\n",
            "Epoch 380/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 7.4559e-04 - accuracy: 1.0000\n",
            "Epoch 380: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 3.0092 - val_accuracy: 0.8000\n",
            "Epoch 381/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 381: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0476 - accuracy: 0.9875 - val_loss: 2.9114 - val_accuracy: 0.8500\n",
            "Epoch 382/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0999 - accuracy: 0.9667\n",
            "Epoch 382: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0848 - accuracy: 0.9750 - val_loss: 2.7323 - val_accuracy: 0.8500\n",
            "Epoch 383/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 383: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0274 - accuracy: 0.9937 - val_loss: 2.6692 - val_accuracy: 0.8750\n",
            "Epoch 384/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 384: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0235 - accuracy: 0.9937 - val_loss: 2.6480 - val_accuracy: 0.8750\n",
            "Epoch 385/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 385: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0465 - accuracy: 0.9812 - val_loss: 2.6629 - val_accuracy: 0.8750\n",
            "Epoch 386/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0535 - accuracy: 1.0000\n",
            "Epoch 386: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 2.6013 - val_accuracy: 0.8750\n",
            "Epoch 387/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 387: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0437 - accuracy: 0.9875 - val_loss: 2.5414 - val_accuracy: 0.8750\n",
            "Epoch 388/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 388: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 2.5218 - val_accuracy: 0.8750\n",
            "Epoch 389/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 389: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 2.5310 - val_accuracy: 0.8750\n",
            "Epoch 390/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
            "Epoch 390: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0538 - accuracy: 0.9812 - val_loss: 2.5460 - val_accuracy: 0.8750\n",
            "Epoch 391/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 391: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 2.5614 - val_accuracy: 0.8750\n",
            "Epoch 392/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 392: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0527 - accuracy: 0.9875 - val_loss: 2.5847 - val_accuracy: 0.8750\n",
            "Epoch 393/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 4.5882e-04 - accuracy: 1.0000\n",
            "Epoch 393: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.6265 - val_accuracy: 0.8500\n",
            "Epoch 394/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2993 - accuracy: 0.9667\n",
            "Epoch 394: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0734 - accuracy: 0.9937 - val_loss: 2.7937 - val_accuracy: 0.8750\n",
            "Epoch 395/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.1093 - accuracy: 0.9667\n",
            "Epoch 395: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0933 - accuracy: 0.9750 - val_loss: 2.8647 - val_accuracy: 0.8750\n",
            "Epoch 396/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0923 - accuracy: 0.9667\n",
            "Epoch 396: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0812 - accuracy: 0.9625 - val_loss: 2.8952 - val_accuracy: 0.8750\n",
            "Epoch 397/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 397: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.0452 - accuracy: 0.9750 - val_loss: 2.9814 - val_accuracy: 0.8750\n",
            "Epoch 398/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
            "Epoch 398: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0529 - accuracy: 0.9688 - val_loss: 3.1572 - val_accuracy: 0.8500\n",
            "Epoch 399/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.2192 - accuracy: 0.9000\n",
            "Epoch 399: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1465 - accuracy: 0.9563 - val_loss: 3.0946 - val_accuracy: 0.8500\n",
            "Epoch 400/400\n",
            "1/6 [====>.........................] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 400: val_loss did not improve from 1.99957\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0432 - accuracy: 0.9875 - val_loss: 2.8499 - val_accuracy: 0.8750\n",
            "Training completed in time  0:00:37.470068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy= model.evaluate(X_test, y_test, verbose=0)\n",
        "print(test_accuracy[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vic0LTMWM9FT",
        "outputId": "ca72a26b-7163-4aaa-f5d5-4d44370e5bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "GyVXQXgKJ64V",
        "outputId": "d5b6961d-c224-41fb-912d-894ecaaa9e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.src.engine.sequential.Sequential"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.engine.sequential.Sequential</b><br/>def error_handler(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py</a>`Sequential` groups a linear stack of layers into a `tf.keras.Model`.\n",
              "\n",
              "`Sequential` provides training and inference features on this model.\n",
              "\n",
              "Examples:\n",
              "\n",
              "```python\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.Input(shape=(16,)))\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "\n",
              "# Note that you can also omit the initial `Input`.\n",
              "# In that case the model doesn&#x27;t have any weights until the first call\n",
              "# to a training/evaluation method (since it isn&#x27;t yet built):\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "model.add(tf.keras.layers.Dense(4))\n",
              "# model.weights not created yet\n",
              "\n",
              "# Whereas if you specify an `Input`, the model gets built\n",
              "# continuously as you are adding layers:\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.Input(shape=(16,)))\n",
              "model.add(tf.keras.layers.Dense(4))\n",
              "len(model.weights)\n",
              "# Returns &quot;2&quot;\n",
              "\n",
              "# When using the delayed-build pattern (no input shape specified), you can\n",
              "# choose to manually build your model by calling\n",
              "# `build(batch_input_shape)`:\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "model.add(tf.keras.layers.Dense(4))\n",
              "model.build((None, 16))\n",
              "len(model.weights)\n",
              "# Returns &quot;4&quot;\n",
              "\n",
              "# Note that when using the delayed-build pattern (no input shape specified),\n",
              "# the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
              "# or the first time you call the model on some input data.\n",
              "model = tf.keras.Sequential()\n",
              "model.add(tf.keras.layers.Dense(8))\n",
              "model.add(tf.keras.layers.Dense(1))\n",
              "model.compile(optimizer=&#x27;sgd&#x27;, loss=&#x27;mse&#x27;)\n",
              "# This builds the model for the first time:\n",
              "model.fit(x, y, batch_size=32, epochs=10)\n",
              "```</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 47);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDbjXOrpJuR6",
        "outputId": "404365ea-062e-4662-82b3-a118ba6abe6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "model.save('dog_audio_model_new.hdf5')"
      ],
      "metadata": {
        "id": "8_BalbsuJgwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class labels\n",
        "class_labels = [\"angry\", \"happy\", \"howling\", \"sad\"]\n",
        "\n",
        "file1= path+'/happy_40.wav'\n",
        "prediction_feature= features_extractor(file1)\n",
        "prediction_feature= prediction_feature.reshape(1,-1)\n",
        "predicted_probabilities = model.predict(prediction_feature)\n",
        "predicted_class = np.argmax(predicted_probabilities)\n",
        "\n",
        "# Get the predicted class label\n",
        "predicted_label = class_labels[predicted_class]\n",
        "\n",
        "# Print the predicted class label\n",
        "print(\"Predicted class:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo8paxI_L7OJ",
        "outputId": "2f7a1eb7-08d6-4e1d-ff27-b0e5fa96c59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 607ms/step\n",
            "Predicted class: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing some test audio data\n",
        "Steps\n",
        "\n",
        "*   Preprocess the new audio\n",
        "*   Predict the class\n",
        "*   Inverse transform the predicted label\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CXgMdS_E_cSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file2= \"/content/drive/MyDrive/Colab_Datasets/extra_audio/happy14 (10).wav\"\n",
        "audio, sample_rate= librosa.load(file2, res_type=\"kaiser_fast\")\n",
        "mfccs_features= librosa.feature.mfcc(y=audio, sr= sample_rate, n_mfcc=40)\n",
        "mfccs_scaled_features= np.mean(mfccs_features.T, axis=0)\n",
        "\n",
        "print(mfccs_scaled_features)\n",
        "mfccs_scaled_features= mfccs_scaled_features.reshape(1, -1)\n",
        "print(mfccs_scaled_features)\n",
        "print(mfccs_scaled_features.shape)\n",
        "predicted_probabilities = model.predict(mfccs_scaled_features)\n",
        "predicted_label = np.argmax(predicted_probabilities)\n",
        "labelencoder= LabelEncoder()\n",
        "labelencoder.fit(class_labels)\n",
        "predicted_class = labelencoder.inverse_transform([predicted_label])[0]\n",
        "print(predicted_class)"
      ],
      "metadata": {
        "id": "tdy2QQ85XVmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10bf37f-fd2c-475f-cb96-4253cc6b8869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.9960861e+02  4.1762726e+01 -7.8915611e+01 -2.3142332e+01\n",
            " -4.5388935e+01 -1.7579777e+01 -4.4500213e+00  2.2316996e+01\n",
            " -5.5414019e+00 -1.0038956e+00 -1.7447479e+01 -8.3142824e+00\n",
            "  5.5894876e-01  4.7041802e+00 -1.1429214e+01 -4.7386274e+00\n",
            " -1.2175715e+01  3.3181965e+00  1.3426679e+00 -1.5277928e+00\n",
            " -3.5320046e+00 -2.1987154e+00 -3.9321747e+00 -1.8355829e-01\n",
            "  4.3296580e+00  4.6258240e+00  9.9623424e-01  2.9667406e+00\n",
            " -6.6171247e-01 -3.6010184e+00  3.8335693e-01  5.8403856e-01\n",
            "  1.2995818e+00 -7.2957313e-01 -7.9670370e-01 -9.4689333e-01\n",
            "  1.0646731e+00  6.1729956e-01  1.4543340e+00  1.5793740e+00]\n",
            "[[-2.9960861e+02  4.1762726e+01 -7.8915611e+01 -2.3142332e+01\n",
            "  -4.5388935e+01 -1.7579777e+01 -4.4500213e+00  2.2316996e+01\n",
            "  -5.5414019e+00 -1.0038956e+00 -1.7447479e+01 -8.3142824e+00\n",
            "   5.5894876e-01  4.7041802e+00 -1.1429214e+01 -4.7386274e+00\n",
            "  -1.2175715e+01  3.3181965e+00  1.3426679e+00 -1.5277928e+00\n",
            "  -3.5320046e+00 -2.1987154e+00 -3.9321747e+00 -1.8355829e-01\n",
            "   4.3296580e+00  4.6258240e+00  9.9623424e-01  2.9667406e+00\n",
            "  -6.6171247e-01 -3.6010184e+00  3.8335693e-01  5.8403856e-01\n",
            "   1.2995818e+00 -7.2957313e-01 -7.9670370e-01 -9.4689333e-01\n",
            "   1.0646731e+00  6.1729956e-01  1.4543340e+00  1.5793740e+00]]\n",
            "(1, 40)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kSKlyYUu5PY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}